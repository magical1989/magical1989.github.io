{"meta":{"title":"Hexo","subtitle":null,"description":null,"author":"Liu Xi","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"","slug":"bigdata/spark/Spark Streaming 源码解析系列/readme","date":"2018-02-09T03:49:13.960Z","updated":"2018-02-09T03:48:41.442Z","comments":true,"path":"2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/readme/","link":"","permalink":"http://yoursite.com/2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/readme/","excerpt":"","text":"Spark Streaming 源码解析系列「腾讯·广点通」技术团队荣誉出品 12345本系列内容适用范围：* 2017.07.11 update, Spark 2.2 全系列 √ (已发布：2.2.0)* 2017.10.02 update, Spark 2.1 全系列 √ (已发布：2.1.0, 2.1.1, 2.1.2)* 2016.11.14 update, Spark 2.0 全系列 √ (已发布：2.0.0, 2.0.1, 2.0.2) 概述 0.1 Spark Streaming 实现思路与模块概述 模块 1：DAG 静态定义 1.1 DStream, DStreamGraph 详解 1.2 DStream 生成 RDD 实例详解 模块 2：Job 动态生成 2.1 JobScheduler, Job, JobSet 详解 2.2 JobGenerator 详解 模块 3：数据产生与导入 3.1 Receiver 分发详解 3.2 Receiver, ReceiverSupervisor, BlockGenerator, ReceivedBlockHandler 详解 3.3 ReceiverTraker, ReceivedBlockTracker 详解 模块 4：长时容错 4.1 Executor 端长时容错详解 4.2 Driver 端长时容错详解 StreamingContext 5.1 StreamingContext 详解 一些资源和 Q&amp;A Spark 资源集合 (包括 Spark Summit 视频，Spark 中文微信群等资源集合) (Q&amp;A) 什么是 end-to-end exactly-once? 致谢 Github @wongxingjun 同学指出 3 处 typo，并提 Pull Request 修正（PR 已合并） Github @endymecy 同学指出 2 处 typo，并提 Pull Request 修正（PR 已合并） Github @xiaoguoqiang 同学指出 1 处 typo，并提 Pull Request 修正（PR 已合并） Github 张瀚 (@AntikaSmith) 同学指出 1 处 问题（已修正） Github Tao Meng (@mtunique) 同学指出 1 处 typo，并提 Pull Request 修正（PR 已合并） Github @ouyangshourui 同学指出 1 处问题，并提 Pull Request 修正（PR 已合并） Github @jacksu 同学指出 1 处问题，并提 Pull Request 修正（PR 已合并） Github @klion26 同学指出 1 处 typo（已修正） Github @397090770 同学指出 1 处配图笔误（已修正） Github @ubtaojiang1982 同学指出 1 处 typo（已修正） Github @marlin5555 同学指出 1 处配图遗漏信息（已修正） Weibo @wyggggo 同学指出 1 处 typo（已修正） Spark Streaming 史前史(1)作为跑在商业硬件上的大数据处理框架，Apache Hadoop 在诞生后的几年内（2005~今）火的一塌糊涂，几乎成为了业界处理大数据的事实上的标准工具： Spark Streaming 史前史(2)不过大家逐渐发现还需要有单独针对流式数据（其特点是源数据实时性高，要求处理延迟低）的处理需求；于是自 2010 年起又流行起了很多通用流数据处理框架，这种与 Hadoop 等批处理框架配合使用的“批+实时”的双引擎架构又成为了当前事实上的标准： ps: 前段时间跟一位前 Googler（很巧他是 MillWheel 的第一批用户）一起吃饭时，了解到 MillWheel 原来是 2010 年左右开发的，据说极其极其好用。 Spark Streaming 诞生 本系列文章，就来详解发布于 2013 年的 Spark Streaming。 知识共享 除非另有注明，本《Spark Streaming 源码解析系列》系列文章使用 CC BY-NC（署名-非商业性使用） 知识共享许可协议。","categories":[],"tags":[]},{"title":"","slug":"bigdata/spark/Spark Streaming 源码解析系列/Q&A 什么是 end-to-end exactly-once","date":"2018-02-09T03:49:13.954Z","updated":"2018-02-09T03:48:41.440Z","comments":true,"path":"2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/Q&A 什么是 end-to-end exactly-once/","link":"","permalink":"http://yoursite.com/2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/Q&A 什么是 end-to-end exactly-once/","excerpt":"","text":"[Q] 什么是 end-to-end exactly-once ?[A] 一般我们把上游数据源 (Source) 看做一个 end，把下游数据接收 (Sink) 看做另一个 end： 12Source --&gt; Spark Streaming --&gt; Sink[end] [end] 目前的 Spark Streaming 处理过程自身是 exactly-once 的，而且对上游这个 end 的数据管理做得也不错（比如在 direct 模式里自己保存 Kafka 的偏移），但对下游除 HDFS 外的如 HBase, MySQL, Redis 等诸多 end 还不太友好，需要 user code 来实现幂等逻辑、才能保证 end-to-end 的 exactly-once。 而在 Spark 2.0 引入的 Structured Streaming 里，将把常见的下游 end 也管理起来（比如通过 batch id 来原生支持幂等），那么不需要 user code 做什么就可以保证 end-to-end 的 exactly-once 了，请见下面一张来自 databricks 的 slide[1]: [1] Reynold Xin (Databricks), “the Future of Real-time in Spark”, 2016.02, http://www.slideshare.net/rxin/the-future-of-realtime-in-spark. – （本文完，参与本文的讨论请 猛戳这里，返回目录请 猛戳这里）","categories":[],"tags":[]},{"title":"","slug":"bigdata/spark/Spark Streaming 源码解析系列/4.2 Driver 端长时容错详解","date":"2018-02-09T03:49:13.946Z","updated":"2018-02-09T03:48:41.439Z","comments":true,"path":"2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/4.2 Driver 端长时容错详解/","link":"","permalink":"http://yoursite.com/2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/4.2 Driver 端长时容错详解/","excerpt":"","text":"Driver 端长时容错详解[酷玩 Spark] Spark Streaming 源码解析系列 ，返回目录请 猛戳这里 「腾讯·广点通」技术团队荣誉出品 12345本系列内容适用范围：* 2017.07.11 update, Spark 2.2 全系列 √ (已发布：2.2.0)* 2017.10.02 update, Spark 2.1 全系列 √ (已发布：2.1.0, 2.1.1, 2.1.2)* 2016.11.14 update, Spark 2.0 全系列 √ (已发布：2.0.0, 2.0.1, 2.0.2) 阅读本文前，请一定先阅读 Spark Streaming 实现思路与模块概述 一文，其中概述了 Spark Streaming 的 4 大模块的基本作用，有了全局概念后再看本文对 模块 4：长时容错 细节的解释。 引言之前的详解我们详解了完成 Spark Streamimg 基于 Spark Core 所新增功能的 3 个模块，接下来我们看一看第 4 个模块将如何保障 Spark Streaming 的长时运行 —— 也就是，如何与前 3 个模块结合，保障前 3 个模块的长时运行。 通过前 3 个模块的关键类的分析，我们可以知道，保障模块 1 和 2 需要在 driver 端完成，保障模块 3 需要在 executor 端和 driver 端完成。 本文我们详解 driver 端的保障。具体的，包括两部分： (1) ReceivedBlockTracker 容错 采用 WAL 冷备方式 (2) DStream, JobGenerator 容错 采用 Checkpoint 冷备方式 (1) ReceivedBlockTracker 容错详解前面我们讲过，块数据的 meta 信息上报到 ReceiverTracker，然后交给 ReceivedBlockTracker 做具体的管理。ReceivedBlockTracker 也采用 WAL 冷备方式进行备份，在 driver 失效后，由新的 ReceivedBlockTracker 读取 WAL 并恢复 block 的 meta 信息。 WriteAheadLog 的方式在单机 RDBMS、NoSQL/NewSQL 中都有广泛应用，前者比如记录 transaction log 时，后者比如 HBase 插入数据可以先写到 HLog 里。 WriteAheadLog 的特点是顺序写入，所以在做数据备份时效率较高，但在需要恢复数据时又需要顺序读取，所以需要一定 recovery time。 WriteAheadLog 及其基于 rolling file 的实现 FileBasedWriteAheadLog 我们在 Executor 端长时容错详解 详解过了，下面我们主要看 ReceivedBlockTracker 如何使用 WAL。 ReceivedBlockTracker 里有一个 writeToLog() 方法，会将具体的 log 信息写到 rolling log 里。我们看代码有哪些地方用到了 writeToLog()： 12345678910111213141516171819202122232425262728def addBlock(receivedBlockInfo: ReceivedBlockInfo): Boolean = synchronized &#123; ... // 【在收到了 Receiver 报上来的 meta 信息后，先通过 writeToLog() 写到 WAL】 writeToLog(BlockAdditionEvent(receivedBlockInfo)) // 【再将 meta 信息索引起来】 getReceivedBlockQueue(receivedBlockInfo.streamId) += receivedBlockInfo ...&#125;def allocateBlocksToBatch(batchTime: Time): Unit = synchronized &#123; ... // 【在收到了 JobGenerator 的为最新的 batch 划分 meta 信息的要求后，先通过 writeToLog() 写到 WAL】 writeToLog(BatchAllocationEvent(batchTime, allocatedBlocks)) // 【再将 meta 信息划分到最新的 batch 里】 timeToAllocatedBlocks(batchTime) = allocatedBlocks ...&#125;def cleanupOldBatches(cleanupThreshTime: Time, waitForCompletion: Boolean): Unit = synchronized &#123; ... // 【在收到了 JobGenerator 的清除过时的 meta 信息要求后，先通过 writeToLog() 写到 WAL】 writeToLog(BatchCleanupEvent(timesToCleanup)) // 【再将过时的 meta 信息清理掉】 timeToAllocatedBlocks --= timesToCleanup // 【再将 WAL 里过时的 meta 信息对应的 log 清理掉】 writeAheadLogOption.foreach(_.clean(cleanupThreshTime.milliseconds, waitForCompletion))&#125; 通过上面的代码可以看到，有 3 种消息 —— BlockAdditionEvent, BatchAllocationEvent, BatchCleanupEvent —— 会被保存到 WAL 里。 也就是，如果我们从 WAL 中恢复，能够拿到这 3 种消息，然后从头开始重做这些 log，就能重新构建出 ReceivedBlockTracker 的状态成员： (2) DStream, JobGenerator 容错详解另外，需要定时对 DStreamGraph 和 JobScheduler 做 Checkpoint，来记录整个 DStreamGraph 的变化、和每个 batch 的 job 的完成情况。 注意到这里采用的是完整 checkpoint 的方式，和之前的 WAL 的方式都不一样。Checkpoint 通常也是落地到可靠存储如 HDFS。Checkpoint 发起的间隔默认的是和 batchDuration 一致；即每次 batch 发起、提交了需要运行的 job 后就做 Checkpoint，另外在 job 完成了更新任务状态的时候再次做一下 Checkpoint。 具体的，JobGenerator.doCheckpoint() 实现是，new 一个当前状态的 Checkpoint，然后通过 CheckpointWriter 写出去： 12345678910// 来自 JobGeneratorprivate def doCheckpoint(time: Time, clearCheckpointDataLater: Boolean) &#123; if (shouldCheckpoint &amp;&amp; (time - graph.zeroTime).isMultipleOf(ssc.checkpointDuration)) &#123; logInfo(\"Checkpointing graph for time \" + time) ssc.graph.updateCheckpointData(time) // 【new 一个当前状态的 Checkpoint，然后通过 CheckpointWriter 写出去】 checkpointWriter.write(new Checkpoint(ssc, time), clearCheckpointDataLater) &#125;&#125; 然后我们看 JobGenerator.doCheckpoint() 在哪里被调用： 123456789101112// 来自 JobGeneratorprivate def processEvent(event: JobGeneratorEvent) &#123; logDebug(\"Got event \" + event) event match &#123; ... // 【是异步地收到 DoCheckpoint 消息后，在一个线程池里执行 doCheckpoint() 方法】 case DoCheckpoint(time, clearCheckpointDataLater) =&gt; doCheckpoint(time, clearCheckpointDataLater) ... &#125;&#125; 所以进一步看，到底哪里发送过 DoCheckpoint 消息： 123456789101112131415161718192021222324252627// 来自 JobGeneratorprivate def generateJobs(time: Time) &#123; SparkEnv.set(ssc.env) Try &#123; jobScheduler.receiverTracker.allocateBlocksToBatch(time) // 【步骤 (1)】 graph.generateJobs(time) // 【步骤 (2)】 &#125; match &#123; case Success(jobs) =&gt; val streamIdToInputInfos = jobScheduler.inputInfoTracker.getInfo(time) // 【步骤 (3)】 jobScheduler.submitJobSet(JobSet(time, jobs, streamIdToInputInfos)) // 【步骤 (4)】 case Failure(e) =&gt; jobScheduler.reportError(\"Error generating jobs for time \" + time, e) &#125; eventLoop.post(DoCheckpoint(time, clearCheckpointDataLater = false)) // 【步骤 (5)】&#125;// 来自 JobSchedulerprivate def clearMetadata(time: Time) &#123; ssc.graph.clearMetadata(time) if (shouldCheckpoint) &#123; // 【一个 batch 做完，需要 clean 元数据时】 eventLoop.post(DoCheckpoint(time, clearCheckpointDataLater = true)) &#125; ...&#125; 原来是两处会发送 DoCheckpoint 消息： 第 1 处就是经典的 JobGenerator.generateJob() 的第 (5) 步 是在第 (4) 步提交了 JobSet 给 JobScheduler 异步执行后，就马上执行第 (5) 步来发送 DoCheckpoint 消息（如下图） 第 2 处是 JobScheduler 成功执行完了提交过来的 JobSet 后，就可以清除此 batch 的相关信息了 这时是先 clear 各种信息 然后发送 DoCheckpoint 消息，触发 doCheckpoint()，就会记录下来我们已经做完了一个 batch 解决了什么时候 doCheckpoint()，现在唯一的问题就是 Checkpoint 都会包含什么内容了。 Checkpoint 详解我们看看 Checkpoint 的具体内容，整个列表如下： 123456789101112来自 Checkpointval checkpointTime: Timeval master: String = ssc.sc.masterval framework: String = ssc.sc.appNameval jars: Seq[String] = ssc.sc.jarsval graph: DStreamGraph = ssc.graph // 【重要】val checkpointDir: String = ssc.checkpointDirval checkpointDuration: Duration = ssc.checkpointDurationval pendingTimes: Array[Time] = ssc.scheduler.getPendingTimes().toArray // 【重要】val delaySeconds: Int = MetadataCleaner.getDelaySeconds(ssc.conf)val sparkConfPairs: Array[(String, String)] = ssc.conf.getAll （本文完，参与本文的讨论请 猛戳这里，返回目录请 猛戳这里）","categories":[],"tags":[]},{"title":"","slug":"bigdata/spark/Spark Streaming 源码解析系列/4.1 Executor 端长时容错详解","date":"2018-02-09T03:49:13.940Z","updated":"2018-02-09T03:48:41.438Z","comments":true,"path":"2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/4.1 Executor 端长时容错详解/","link":"","permalink":"http://yoursite.com/2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/4.1 Executor 端长时容错详解/","excerpt":"","text":"Executor 端长时容错详解[酷玩 Spark] Spark Streaming 源码解析系列 ，返回目录请 猛戳这里 「腾讯·广点通」技术团队荣誉出品 12345本系列内容适用范围：* 2017.07.11 update, Spark 2.2 全系列 √ (已发布：2.2.0)* 2017.10.02 update, Spark 2.1 全系列 √ (已发布：2.1.0, 2.1.1, 2.1.2)* 2016.11.14 update, Spark 2.0 全系列 √ (已发布：2.0.0, 2.0.1, 2.0.2) 阅读本文前，请一定先阅读 Spark Streaming 实现思路与模块概述 一文，其中概述了 Spark Streaming 的 4 大模块的基本作用，有了全局概念后再看本文对 模块 4：长时容错 细节的解释。 引言之前的详解我们详解了完成 Spark Streamimg 基于 Spark Core 所新增功能的 3 个模块，接下来我们看一看第 4 个模块将如何保障 Spark Streaming 的长时运行 —— 也就是，如何与前 3 个模块结合，保障前 3 个模块的长时运行。 通过前 3 个模块的关键类的分析，我们可以知道，保障模块 1 和 2 需要在 driver 端完成，保障模块 3 需要在 executor 端和 driver 端完成。 本文我们详解 executor 端的保障。 在 executor 端，ReceiverSupervisor 和 Receiver 失效后直接重启就 OK 了，关联是保障收到的块数据的安全。保障了源头块数据，就能够保障 RDD DAG （Spark Core 的 lineage）重做。 Spark Streaming 对源头块数据的保障，分为 4 个层次，全面、相互补充，又可根据不同场景灵活设置： (1) 热备 (2) 冷备 (3) 重放 (4) 忽略 (1) 热备热备是指在存储块数据时，将其存储到本 executor、并同时 replicate 到另外一个 executor 上去。这样在一个 replica 失效后，可以立刻无感知切换到另一份 replica 进行计算。 实现方式是，在实现自己的 Receiver 时，即指定一下 StorageLevel 为 MEMORY_ONLY_2 或 MEMORY_AND_DISK_2 就可以了。 比如这样： 1234class MyReceiver extends Receiver(StorageLevel.MEMORY_ONLY_2) &#123; override def onStart(): Unit = &#123;&#125; override def onStop(): Unit = &#123;&#125;&#125; 这样，Receiver 在将数据 store() 给 ReceiverSupervisorImpl 的时候，将同时指明此 storageLevel。ReceiverSupervisorImpl 也将根据此 storageLevel 将块数据具体的存储给 BlockManager。 然后就是依靠 BlockManager 进行热备。具体的 —— 我们以 ReceiverSupervisorImpl 向 BlockManager 存储一个 byteBuffer 为例 —— BlockManager 在收到 putBytes(byteBuffer) 时，实际是直接调用 doPut(byteBuffer) 的。 那么我们看 doPut(...) 方法（友情提醒，主要看代码里的注释）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354private def doPut(blockId: BlockId, data: BlockValues, level: StorageLevel, ...) : Seq[(BlockId, BlockStatus)] = &#123; ... //【如果 putLevel.replication &gt; 1 的话，就定义这个 future，复制数据到另外的 executor 上】 val replicationFuture = data match &#123; case b: ByteBufferValues if putLevel.replication &gt; 1 =&gt; val bufferView = b.buffer.duplicate() Future &#123; //【这里非常重要，会在 future 启动时去实际调用 replicate() 方法，复制数据到另外的 executor 上】 replicate(blockId, bufferView, putLevel) &#125;(futureExecutionContext) case _ =&gt; null &#125; putBlockInfo.synchronized &#123; ... // 【存储到本机 blockManager 的 blockStore 里】 val result = data match &#123; case IteratorValues(iterator) =&gt; blockStore.putIterator(blockId, iterator, putLevel, returnValues) case ArrayValues(array) =&gt; blockStore.putArray(blockId, array, putLevel, returnValues) case ByteBufferValues(bytes) =&gt; bytes.rewind() blockStore.putBytes(blockId, bytes, putLevel) &#125; &#125; //【再次判断 putLevel.replication &gt; 1】 if (putLevel.replication &gt; 1) &#123; data match &#123; case ByteBufferValues(bytes) =&gt; //【如果之前启动了 replicate 的 future，那么这里就同步地等这个 future 结束】 if (replicationFuture != null) &#123; Await.ready(replicationFuture, Duration.Inf) &#125; case _ =&gt; val remoteStartTime = System.currentTimeMillis if (bytesAfterPut == null) &#123; if (valuesAfterPut == null) &#123; throw new SparkException( \"Underlying put returned neither an Iterator nor bytes! This shouldn't happen.\") &#125; bytesAfterPut = dataSerialize(blockId, valuesAfterPut) &#125; //【否则之前没有启动 replicate 的 future，那么这里就同步地调用 replicate() 方法，复制数据到另外的 executor 上】 replicate(blockId, bytesAfterPut, putLevel) logDebug(\"Put block %s remotely took %s\" .format(blockId, Utils.getUsedTimeMs(remoteStartTime))) &#125; &#125; ...&#125; 所以，可以看到， BlockManager 的 putBytes() 语义就是承诺了，如果指定需要 replicate，那么当 putBytes() 方法返回时，就一定是存储到本机、并且一定 replicate 到另外的 executor 上了。对于 BlockManager 的 putIterator() 也是同样的语义，因为 BlockManager 的 putIterator() 和 BlockManager 的 putBytes() 一样，都是基于 BlockManager 的 doPut() 来实现的。 简单总结本小节的解析，Receiver 收到的数据，通过 ReceiverSupervisorImpl，将数据交给 BlockManager 存储；而 BlockManager 本身支持将数据 replicate() 到另外的 executor 上，这样就完成了 Receiver 源头数据的热备过程。 而在计算时，计算任务首先将获取需要的块数据，这是如果一个 executor 失效导致一份数据丢失，那么计算任务将转而向另一个 executor 上的同一份数据获取数据。因为另一份块数据是现成的、不需要像冷备那样重新读取的，所以这里不会有 recovery time。 (2) 冷备!!! 需要同时修改 冷备是每次存储块数据时，除了存储到本 executor，还会把块数据作为 log 写出到 WriteAheadLog 里作为冷备。这样当 executor 失效时，就由另外的 executor 去读 WAL，再重做 log 来恢复块数据。WAL 通常写到可靠存储如 HDFS 上，所以恢复时可能需要一段 recover time。 冷备的写出过程如下图 4(c) 过程所示： 这里我们需要插播一下详解 WriteAheadLog 框架。 WriteAheadLog 框架WriteAheadLog 的方式在单机 RDBMS、NoSQL/NewSQL 中都有广泛应用，前者比如记录 transaction log 时，后者比如 HBase 插入数据可以先写到 HLog 里。 WriteAheadLog 的特点是顺序写入，所以在做数据备份时效率较高，但在需要恢复数据时又需要顺序读取，所以需要一定 recovery time。 不过对于 Spark Streaming 的块数据冷备来讲，在恢复时也非常方便。这是因为，对某个块数据的操作只有一次（即新增块数据），而没有后续对块数据的追加、修改、删除操作，这就使得在 WAL 里只会有一条此块数据的 log entry。所以，我们在恢复时只要 seek 到这条 log entry 并读取就可以了，而不需要顺序读取整个 WAL。 也就是，Spark Streaming 基于 WAL 冷备进行恢复，需要的 recovery time 只是 seek 到并读一条 log entry 的时间，而不是读取整个 WAL 的时间，这个是个非常大的节省。 Spark Streaming 里的 WAL 框架，由一组抽象类，和一组基于文件的具体实现组成。其类结构关系如下： WriteAheadLog, WriteAheadLogRecordHandleWriteAheadLog 是多条 log 的集合，每条具体的 log 的引用就是一个 LogRecordHandle。这两个 abstract 的接口定义如下： 1234567891011121314151617181920212223242526// 来自 WriteAheadLog@org.apache.spark.annotation.DeveloperApipublic abstract class WriteAheadLog &#123; // 【写方法：写入一条 log，将返回一个指向这条 log 的句柄引用】 abstract public WriteAheadLogRecordHandle write(ByteBuffer record, long time); // 【读方法：给定一条 log 的句柄引用，读出这条 log】 abstract public ByteBuffer read(WriteAheadLogRecordHandle handle); // 【读方法：读取全部 log】 abstract public Iterator&lt;ByteBuffer&gt; readAll(); // 【清理过时的 log 条目】 abstract public void clean(long threshTime, boolean waitForCompletion); // 【关闭方法】 abstract public void close();&#125;// 来自 WriteAheadLogRecordHandle@org.apache.spark.annotation.DeveloperApipublic abstract class WriteAheadLogRecordHandle implements java.io.Serializable &#123; // 【Handle 则是一个空接口，需要具体的子类定义真正的内容】&#125; 这里 WriteAheadLog 基于文件的具体实现是 FileBasedWriteAheadLog，WriteAheadLogRecordHandle 基于文件的具体实现是 FileBasedWriteAheadLogSegment，下面我们详细看看这两个具体的类。 FileBasedWriteAheadLogSegmentFileBasedWriteAheadLog 有 3 个重要的配置项或成员： rolling 配置项 FileBasedWriteAheadLog 的实现把 log 写到一个文件里（一般是 HDFS 等可靠存储上的文件），然后每隔一段时间就关闭已有文件，产生一些新文件继续写，也就是 rolling 写的方式 rolling 写的好处是单个文件不会太大，而且删除不用的旧数据特别方便 这里 rolling 的间隔是由参数 spark.streaming.receiver.writeAheadLog.rollingIntervalSecs（默认 = 60 秒） 控制的 WAL 存放的目录：{checkpointDir}/receivedData/{receiverId} {checkpointDir} 在 ssc.checkpoint(checkpointDir) 指定的 {receiverId} 是 Receiver 的 id 在这个 WAL 目录里，不同的 rolling log 文件的命名规则是 log-{startTime}-{stopTime} 然后就是 FileBasedWriteAheadLog.currentLogWriter 一个 LogWriter 对应一个 log file，而且 log 文件本身是 rolling 的，那么前一个 log 文件写完成后，对应的 writer 就可以 close() 了，而由新的 writer 负责写新的文件 这里最新的 LogWriter 就由 currentLogWriter 来指向 接下来就是 FileBasedWriteAheadLog 的读写方法了： write(byteBuffer: ByteBuffer, time: Long) 最重要的是先调用 getCurrentWriter()，获取当前的 currentWriter 注意这里，如果 log file 需要 rolling 成新的了，那么 currentWriter 也需要随之更新；上面 getCurrentWriter() 会完成这个按需更新 currentWriter 的过程 然后就可以调用 writer.write(byteBuffer) 就可以了 read(segment: WriteAheadLogRecordHandle): ByteBuffer 直接调用 reader.read(fileSegment) 在 reader 的实现里，因为给定了 segment —— 也就是 WriteAheadLogRecordHandle，而 segment 里包含了具体的 log file 和 offset，就可以直接 seek 到这条 log，读出数据并返回 所以总结下可以看到，FileBasedWriteAheadLog 主要是进行 rolling file 的管理，然后将具体的写方法、读方法是由具体的 LogWriter 和 LogReader 来做的。 WriteAheadLogRecordHandle前面我们刚说，WriteAheadLogRecordHandle 是一个 log 句柄的空实现，需要子类指定具体的 log 句柄内容。 然后在基于的 file 的子类实现 WriteAheadLogRecordHandle 里，就记录了 3 方面内容： 1234// 来自 FileBasedWriteAheadLogSegmentprivate[streaming] case class FileBasedWriteAheadLogSegment(path: String, offset: Long, length: Int) extends WriteAheadLogRecordHandle path: String offset: Long length: Int 这 3 方面内容就非常直观了，给定文件、偏移和长度，就可以唯一确定一条 log。 FileBasedWriteAheadLogWriterFileBasedWriteAheadLogWriter 的实现，就是给定一个文件、给定一个块数据，将数据写到文件里面去。 然后在完成的时候，记录一下文件 path、offset 和 length，封装为一个 FileBasedWriteAheadLogSegment 返回。 这里需要注意下的是，在具体的写 HDFS 数据块的时候，需要判断一下具体用的方法，优先使用 hflush()，没有的话就使用 sync()： 1234567// 来自 FileBasedWriteAheadLogWriterprivate lazy val hadoopFlushMethod = &#123; // Use reflection to get the right flush operation val cls = classOf[FSDataOutputStream] Try(cls.getMethod(\"hflush\")).orElse(Try(cls.getMethod(\"sync\"))).toOption&#125; FileBasedWriteAheadLogRandomReaderFileBasedWriteAheadLogRandomReader 的主要方法是 read(segment: FileBasedWriteAheadLogSegment): ByteBuffer，即给定一个 log 句柄，返回一条具体的 log。 这里主要代码如下，注意到其中最关键的是 seek(segment.offset) ! 12345678910111213141516// 来自 FileBasedWriteAheadLogRandomReaderdef read(segment: FileBasedWriteAheadLogSegment): ByteBuffer = synchronized &#123; assertOpen() // 【seek 到这条 log 所在的 offset】 instream.seek(segment.offset) // 【读一下 length】 val nextLength = instream.readInt() HdfsUtils.checkState(nextLength == segment.length, s\"Expected message length to be $&#123;segment.length&#125;, but was $nextLength\") val buffer = new Array[Byte](nextLength) // 【读一下具体的内容】 instream.readFully(buffer) // 【以 ByteBuffer 的形式，返回具体的内容】 ByteBuffer.wrap(buffer)&#125; FileBasedWriteAheadLogReaderFileBasedWriteAheadLogReader 实现跟 FileBasedWriteAheadLogRandomReader 差不多，不过是不需要给定 log 的句柄，而是迭代遍历所有 log： 12345678910111213141516171819202122232425262728293031323334353637// 来自 FileBasedWriteAheadLogReader// 【迭代方法：hasNext()】override def hasNext: Boolean = synchronized &#123; if (closed) &#123; // 【如果已关闭，就肯定不 hasNext 了】 return false &#125; if (nextItem.isDefined) &#123; true &#125; else &#123; try &#123; // 【读出来下一条，如果有，就说明还确实 hasNext】 val length = instream.readInt() val buffer = new Array[Byte](length) instream.readFully(buffer) nextItem = Some(ByteBuffer.wrap(buffer)) logTrace(\"Read next item \" + nextItem.get) true &#125; catch &#123; ... &#125; &#125;&#125;// 【迭代方法：next()】override def next(): ByteBuffer = synchronized &#123; // 【直接返回在 hasNext() 方法里实际读出来的数据】 val data = nextItem.getOrElse &#123; close() throw new IllegalStateException( \"next called without calling hasNext or after hasNext returned false\") &#125; nextItem = None // Ensure the next hasNext call loads new data. data&#125; WAL 总结通过上面几个小节，我们看到，Spark Streaming 有一套基于 rolling file 的 WAL 实现，提供一个写方法，两个读方法： WriteAheadLogRecordHandle write(ByteBuffer record, long time) 由 FileBasedWriteAheadLogWriter 具体实现 ByteBuffer read(WriteAheadLogRecordHandle handle)` 由 FileBasedWriteAheadLogRandomReader 具体实现 Iterator&lt;ByteBuffer&gt; readAll() 由 FileBasedWriteAheadLogReader 具体实现 (3) 重放如果上游支持重放，比如 Apache Kafka，那么就可以选择不用热备或者冷备来另外存储数据了，而是在失效时换一个 executor 进行数据重放即可。 具体的，Spark Streaming 从 Kafka 读取方式有两种： 基于 Receiver 的 这种是将 Kafka Consumer 的偏移管理交给 Kafka —— 将存在 ZooKeeper 里，失效后由 Kafka 去基于 offset 进行重放 这样可能的问题是，Kafka 将同一个 offset 的数据，重放给两个 batch 实例 —— 从而只能保证 at least once 的语义 Direct 方式，不基于 Receiver 由 Spark Streaming 直接管理 offset —— 可以给定 offset 范围，直接去 Kafka 的硬盘上读数据，使用 Spark Streaming 自身的均衡来代替 Kafka 做的均衡 这样可以保证，每个 offset 范围属于且只属于一个 batch，从而保证 exactly-once 这里我们以 Direct 方式为例，详解一下 Spark Streaming 在源头数据实效后，是如果从上游重放数据的。 这里的实现分为两个层面： DirectKafkaInputDStream：负责侦测最新 offset，并将 offset 分配至唯一个 batch 会在每次 batch 生成时，依靠 latestLeaderOffsets() 方法去侦测最新的 offset 然后与上一个 batch 侦测到的 offset 相减，就能得到一个 offset 的范围 offsetRange 把这个 offset 范围内的数据，唯一分配到本 batch 来处理 KafkaRDD：负责去读指定 offset 范围内的数据，并基于此数据进行计算 会生成一个 Kafka 的 SimpleConsumer —— SimpleConsumer 是 Kafka 最底层、直接对着 Kafka 硬盘上的文件读数据的类 如果 Task 失败，导致任务重新下发，那么 offset 范围仍然维持不变，将直接重新生成一个 Kafka 的 SimpleConsumer 去读数据 所以看 Direct 的方式，归根结底是由 Spark Streaming 框架来负责整个 offset 的侦测、batch 分配、实际读取数据；并且这些分 batch 的信息都是 checkpoint 到可靠存储（一般是 HDFS）了。这就没有用到 Kafka 使用 ZooKeeper 来均衡 consumer 和记录 offset 的功能，而是把 Kafka 直接当成一个底层的文件系统来使用了。 当然，我们讲上游重放并不只局限于 Kafka，而是说凡是支持消息重放的上游都可以 —— 比如，HDFS 也可以看做一个支持重放的可靠上游 —— FileInputDStream 就是利用重放的方式，保证了 executor 失效后的源头数据的可读性。 (4) 忽略最后，如果应用的实时性需求大于准确性，那么一块数据丢失后我们也可以选择忽略、不恢复失效的源头数据。 假设我们有 r1, r2, r3 这三个 Receiver，而且每 5 秒产生一个 Block，每 15 秒产生一个 batch。那么，每个 batch 有 15 s ÷ 5 block/s/receiver × 3 receiver = 9 block。现在假设 r1 失效，随之也丢失了 3 个 block。 那么上层应用如何进行忽略？有两种粒度的做法。 粗粒度忽略粗粒度的做法是，如果计算任务试图读取丢失的源头数据时出错，会导致部分 task 计算失败，会进一步导致整个 batch 的 job 失败，最终在 driver 端以 SparkException 的形式报出来 —— 此时我们 catch 住这个 SparkException，就能够屏蔽这个 batch 的 job 失败了。 粗粒度的这个做法实现起来非常简单，问题是会忽略掉整个 batch 的计算结果。虽然我们还有 6 个 block 是好的，但所有 9 个的数据都会被忽略。 细粒度忽略细粒度的做法是，只将忽略部分局限在丢失的 3 个 block 上，其它部分 6 部分继续保留。目前原生的 Spark Streaming 还不能完全做到，但我们对 Spark Streaming 稍作修改，就可以做到了。 细粒度基本思路是，在一个计算的 task 发现作为源数据的 block 失效后，不是直接报错，而是另外生成一个空集合作为“修正”了的源头数据，然后继续 task 的计算，并将成功。 如此一来，仅局限在发生数据丢失的 3 个块数据才会进行“忽略”的过程，6 个好的块数据将正常进行计算。最后整个 job 是成功的。 当然这里对 Spark Streaming 本身的改动，还需要考虑一些细节，比如只在 Spark Streaming 里生效、不要影响到 Spark Core、SparkSQL，再比如 task 通常都是会失效重试的，我们希望前几次现场重试，只在最后一次重试仍不成功的时候再进行忽略。 我们把修改的代码，以及使用方法放在这里了，请随用随取。 总结我们上面分四个小节介绍了 Spark Streaming 对源头数据的高可用的保障方式，我们用一个表格来总结一下： 图示 优点 缺点 (1) 热备 无 recover time 需要占用双倍资源 (2) 冷备 十分可靠 存在 recover time (3) 重放 不占用额外资源 存在 recover time (4) 忽略 无 recover time 准确性有损失 （本文完，参与本文的讨论请 猛戳这里，返回目录请 猛戳这里）","categories":[],"tags":[]},{"title":"","slug":"bigdata/spark/Spark Streaming 源码解析系列/3.3 ReceiverTraker, ReceivedBlockTracker 详解","date":"2018-02-09T03:49:13.930Z","updated":"2018-02-09T03:48:41.428Z","comments":true,"path":"2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/3.3 ReceiverTraker, ReceivedBlockTracker 详解/","link":"","permalink":"http://yoursite.com/2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/3.3 ReceiverTraker, ReceivedBlockTracker 详解/","excerpt":"","text":"ReceiverTraker, ReceivedBlockTracker 详解[酷玩 Spark] Spark Streaming 源码解析系列 ，返回目录请 猛戳这里 「腾讯·广点通」技术团队荣誉出品 12345本系列内容适用范围：* 2017.07.11 update, Spark 2.2 全系列 √ (已发布：2.2.0)* 2017.10.02 update, Spark 2.1 全系列 √ (已发布：2.1.0, 2.1.1, 2.1.2)* 2016.11.14 update, Spark 2.0 全系列 √ (已发布：2.0.0, 2.0.1, 2.0.2) 阅读本文前，请一定先阅读 Spark Streaming 实现思路与模块概述 一文，其中概述了 Spark Streaming 的 4 大模块的基本作用，有了全局概念后再看本文对 模块 3：数据产生与导入 细节的解释。 引言我们在 Spark Streaming 实现思路与模块概述 给出了 模块 3：数据产生与导入 的基本工作流程： (1) 由 Receiver 的总指挥 ReceiverTracker 分发多个 job（每个 job 有 1 个 task），到多个 executor 上分别启动 ReceiverSupervisor 实例； (2) 每个 ReceiverSupervisor 启动后将马上生成一个用户提供的 Receiver 实现的实例 —— 该 Receiver 实现可以持续产生或者持续接收系统外数据，比如 TwitterReceiver 可以实时爬取 twitter 数据 —— 并在 Receiver 实例生成后调用 Receiver.onStart()。 (1)(2) 的过程由上图所示，这时 Receiver 启动工作已运行完毕。 接下来 ReceiverSupervisor 将在 executor 端作为的主要角色，并且： (3) Receiver 在 onStart() 启动后，就将持续不断地接收外界数据，并持续交给 ReceiverSupervisor 进行数据转储； (4) ReceiverSupervisor 持续不断地接收到 Receiver 转来的数据： 如果数据很细小，就需要 BlockGenerator 攒多条数据成一块(4a)、然后再成块存储(4b 或 4c) 反之就不用攒，直接成块存储(4b 或 4c) 这里 Spark Streaming 目前支持两种成块存储方式，一种是由 blockManagerskManagerBasedBlockHandler 直接存到 executor 的内存或硬盘，另一种由 WriteAheadLogBasedBlockHandler 是同时写 WAL(4c) 和 executor 的内存或硬盘 (5) 每次成块在 executor 存储完毕后，ReceiverSupervisor 就会及时上报块数据的 meta 信息给 driver 端的 ReceiverTracker；这里的 meta 信息包括数据的标识 id，数据的位置，数据的条数，数据的大小等信息。 (6) ReceiverTracker 再将收到的块数据 meta 信息直接转给自己的成员 ReceivedBlockTracker，由 ReceivedBlockTracker 专门管理收到的块数据 meta 信息。 这里 (3)(4)(5)(6) 的过程是一直持续不断地发生的，我们也将其在上图里标识出来。 上面的内容我们已经在 Receiver 分发详解 和 Receiver, ReceiverSupervisor, BlockGenerator, ReceivedBlockHandler 详解 中介绍过了。 本文我们详解的是 driver 端的 ReceiverTracker 和 ReceivedBlockTracker ReceiverTracker 的全限定名是：org.apache.spark.streaming.scheduler.ReceiverTracker ReceivedBlockTracker 的全限定名是：org.apache.spark.streaming.scheduler.ReceivedBlockTracker ReceiverTracker 详解ReceiverTracker 在 Spark 1.5.0 版本里的代码变动比较大，不过其主要功能还是没怎么改变，我们一一来看： (1) ReceiverTracker 分发和监控 Receiver ReceiverTracker 负责 Receiver 在各个 executor 上的分发 包括 Receiver 的失败重启 (2) ReceiverTracker 作为 RpcEndpoint ReceiverTracker 作为 Receiver 的管理者，是各个 Receiver 上报信息的入口 也是 driver 下达管理命令到 Receiver 的出口 (3) ReceiverTracker 管理已上报的块数据 meta 信息 整体来看，ReceiverTracker 就是 Receiver 相关信息的中枢。 (1) ReceiverTracker 分发和监控 ReceiverReceiverTracker 分发和监控 Receiver 的内容我们已经在 Receiver 分发详解.md 详解过了，我们这里总结一下。 在 ssc.start() 时，将隐含地调用 ReceiverTracker.start()；而 ReceiverTracker.start() 最重要的任务就是调用自己的 launchReceivers() 方法将 Receiver 分发到多个 executor 上去。然后在每个 executor 上，由 ReceiverSupervisor 来分别启动一个 Receiver 接收数据。这个过程用下图表示： 而且在 1.5.0 版本以来引入了 ReceiverSchedulingPolicy，是在 Spark Streaming 层面添加对 Receiver 的分发目的地的计算，相对于之前版本依赖 Spark Core 的 TaskScheduler 进行通用分发，新的 ReceiverSchedulingPolicy 会对 Streaming 应用的更好的语义理解，也能计算出更好的分发策略。 并且还通过每个 Receiver 对应 1 个 Job 的方式，保证了 Receiver 的多次分发，和失效后的重启、永活。 从本小节 ReceiverTracker 分发和监控 Receiver 的角度，我们对以前版本的 Spark Streaming(以 1.4.0 为代表)、和新版本的 Spark Streaming(以 1.5.0 为代表)有总结对比： Spark Streaming 1.4.0 Spark Streaming 1.5.0 Receiver 活性 不保证永活 无限重试、保证永活 Receiver 均衡分发 无保证 round-robin 策略 自定义 Receiver 分发 很 tricky 方便 (2) ReceiverTracker 作为 RpcEndpointRpcEndPoint 可以理解为 RPC 的 server 端，供 client 调用。 ReceiverTracker 作为 RpcEndPoint 的地址 —— 即 driver 的地址 —— 是公开的，可供 Receiver 连接；如果某个 Receiver 连接成功，那么 ReceiverTracker 也就持有了这个 Receiver 的 RpcEndPoint。这样以来，通过发送消息，就可以实现双向通信。 1.5.0 版本以来，ReceiverTracker 支持的消息有 10 种，我们进行一个总结： StopAllReceivers 消息 消息 解释 ReceiverTracker只接收、不回复 StartAllReceivers 消息 在 ReceiverTracker 刚启动时，发给自己这个消息，触发具体的 schedulingPolicy 计算，和后续分发 RestartReceiver 消息 当初始分发的 executor 不对，或者 Receiver 失效等情况出现，发给自己这个消息，触发 Receiver 重新分发 CleanupOldBlocks 消息 当块数据已完成计算不再需要时，发给自己这个消息，将给所有的 Receiver 转发此 CleanupOldBlocks 消息 UpdateReceiverRateLimit 消息 ReceiverTracker 动态计算出某个 Receiver 新的 rate limit，将给具体的 Receiver 发送 UpdateRateLimit 消息 ReportError 消息 是由 Receiver 上报上来的，将触发 reportError() 方法向 listenerBus 扩散此 error 消息 ReceiverTracker接收并回复 RegisterReceiver 消息 由 Receiver 在试图启动的过程中发来，将回复允许启动，或不允许启动 AddBlock 消息 具体的块数据 meta 上报消息，由 Receiver 发来，将返回成功或失败 DeregisterReceiver 消息 由 Receiver 发来，处理后，无论如何都返回 true AllReceiverIds 消息 在 ReceiverTracker stop() 的过程中，查询是否还有活跃的 Receiver StopAllReceivers 消息 在 ReceiverTracker stop() 的过程刚开始时，要求 stop 所有的 Receiver；将向所有的 Receiver 发送 stop 信息 (3) ReceiverTracker 管理块数据的 meta 信息一方面 Receiver 将通过 AddBlock 消息上报 meta 信息给 ReceiverTracker，另一方面 JobGenerator 将在每个 batch 开始时要求 ReceiverTracker 将已上报的块信息进行 batch 划分，ReceiverTracker 完整了块数据的 meta 信息管理工作。 具体的，ReceiverTracker 有一个成员 ReceivedBlockTracker，专门负责已上报的块数据 meta 信息管理。 ReceivedBlockTracker 详解我们刚刚将，ReceivedBlockTracker 专门负责已上报的块数据 meta 信息管理，但 ReceivedBlockTracker 本身不负责对外交互，一切都是通过 ReceiverTracker 来转发 —— 这里 ReceiverTracker 相当于是 ReceivedBlockTracker 的门面（可参考 门面模式）。 在 ReceivedBlockTracker 内部，有几个重要的成员，它们的关系如下： //TODO(lwlin): 此图风格与本系列文章不符，需要美化 streamIdToUnallocatedBlockQueues 维护了上报上来的、但尚未分配入 batch 的 Block 块数据的 meta 为每个 Receiver 单独维护一个 queue，所以是一个 HashMap：receiverId → mutable.Queue[ReceivedBlockInfo] timeToAllocatedBlocks 维护了上报上来的、已分配入 batch 的 Block 块数据的 meta 按照 batch 进行一级索引、再按照 receiverId 进行二级索引的 queue，所以是一个 HashMap: time → HashMap lastAllocatedBatchTime 记录了最近一个分配完成的 batch 是哪个 上面是用于状态记录的主要数据结构。对这些状态存取主要是 4 个方法： addBlock(receivedBlockInfo: ReceivedBlockInfo) 收到某个 Receiver 上报上来的块数据 meta 信息，将其加入到 streamIdToUnallocatedBlockQueues 里 allocateBlocksToBatch(batchTime: Time) 主要是 JobGenerator 在发起新 batch 的计算时，第一步就调用本方法 是将 streamIdToUnallocatedBlockQueues 的内容，以传入的 batchTime 参数为 key，添加到 timeToAllocatedBlocks 里 并更新 lastAllocatedBatchTime getBlocksOfBatch(batchTime: Time) 主要是 JobGenerator 在发起新 batch 的计算时，由 DStreamGraph 生成 RDD DAG 实例时，将调用本方法 调用本方法查 timeToAllocatedBlocks，获得划入本 batch 的块数据元信息，由此生成处理对应块数据的 RDD cleanupOldBatches(cleanupThreshTime: Time, ...) 主要是当一个 batch 已经计算完成、可以把已追踪的块数据的 meta 信息清理掉时调用 将清理 timeToAllocatedBlocks 表里对应 cleanupThreshTime 之前的所有 batch 块数据 meta 信息 这 4 个方法，和对应信息状态的修改关系如下图总结： //TODO(lwlin): 此图风格与本系列文章不符，需要美化 上面即是 ReceivedBlockTracker 的主体内容。 但我们还需要强调一点非常重要的内容，即 ReceivedBlockTracker 需要对 driver 进行容错保障。也就是，如果 driver 失效，新起来的 driver 必须能够通过 WAL 恢复出失效前的 ReceivedBlockTracker 状态，具体的就需要包括 streamIdToUnallocatedBlockQueues, timeToAllocatedBlocks, lastAllocatedBatchTime 等内容，也即需要前面讲的 4 个方法在修改 ReceivedBlockTracker 的状态信息的时候，要首先写入 WAL，才能在失效后从 WAL 恢复出相关信息。 有关 WAL 写入和故障恢复的内容，我们将在 模块 4：长时容错 里系统性的详解。 总结本文主要详解了 driver 端的 Receiver 管理者 —— ReceiverTracker —— 的主要功能： (1) ReceiverTracker 分发和监控 Receiver ReceiverTracker 负责 Receiver 在各个 executor 上的分发 包括 Receiver 的失败重启 (2) ReceiverTracker 作为 RpcEndpoint ReceiverTracker 作为 Receiver 的管理者，是各个 Receiver 上报信息的入口 也是 driver 下达管理命令到 Receiver 的出口 (3) ReceiverTracker 管理已上报的块数据 meta 信息 委托给自己的成员 ReceivedBlockManager 进行具体管理 （本文完，参与本文的讨论请 猛戳这里，返回目录请 猛戳这里）","categories":[],"tags":[]},{"title":"","slug":"bigdata/spark/Spark Streaming 源码解析系列/3.2 Receiver, ReceiverSupervisor, BlockGenerator, ReceivedBlockHandler 详解","date":"2018-02-09T03:49:13.923Z","updated":"2018-02-09T03:48:41.427Z","comments":true,"path":"2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/3.2 Receiver, ReceiverSupervisor, BlockGenerator, ReceivedBlockHandler 详解/","link":"","permalink":"http://yoursite.com/2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/3.2 Receiver, ReceiverSupervisor, BlockGenerator, ReceivedBlockHandler 详解/","excerpt":"","text":"Receiver, ReceiverSupervisor, BlockGenerator, ReceivedBlockHandler 详解[酷玩 Spark] Spark Streaming 源码解析系列 ，返回目录请 猛戳这里 「腾讯·广点通」技术团队荣誉出品 12345本系列内容适用范围：* 2017.07.11 update, Spark 2.2 全系列 √ (已发布：2.2.0)* 2017.10.02 update, Spark 2.1 全系列 √ (已发布：2.1.0, 2.1.1, 2.1.2)* 2016.11.14 update, Spark 2.0 全系列 √ (已发布：2.0.0, 2.0.1, 2.0.2) 阅读本文前，请一定先阅读 Spark Streaming 实现思路与模块概述 一文，其中概述了 Spark Streaming 的 4 大模块的基本作用，有了全局概念后再看本文对 模块 3：数据产生与导入 细节的解释。 引言我们在前面 Spark Streaming 实现思路与模块概述 中分析过，Spark Streaming 在程序刚开始运行时： (1) 由 Receiver 的总指挥 ReceiverTracker 分发多个 job（每个 job 有 1 个 task），到多个 executor 上分别启动 ReceiverSupervisor 实例； (2) 每个 ReceiverSupervisor 启动后将马上生成一个用户提供的 Receiver 实现的实例 —— 该 Receiver 实现可以持续产生或者持续接收系统外数据，比如 TwitterReceiver 可以实时爬取 twitter 数据 —— 并在 Receiver 实例生成后调用 Receiver.onStart()。 ReceiverSupervisor 的全限定名是：org.apache.spark.streaming.receiver.ReceiverSupervisor Receiver 的全限定名是：org.apache.spark.streaming.receiver.Receiver (1)(2) 的过程由上图所示，这时 Receiver 启动工作已运行完毕。 接下来 ReceiverSupervisor 将在 executor 端作为的主要角色，并且： (3) Receiver 在 onStart() 启动后，就将持续不断地接收外界数据，并持续交给 ReceiverSupervisor 进行数据转储； (4) ReceiverSupervisor 持续不断地接收到 Receiver 转来的数据： 如果数据很细小，就需要 BlockGenerator 攒多条数据成一块(4a)、然后再成块存储(4b 或 4c) 反之就不用攒，直接成块存储(4b 或 4c) 这里 Spark Streaming 目前支持两种成块存储方式，一种是由 blockManagerskManagerBasedBlockHandler 直接存到 executor 的内存或硬盘，另一种由 WriteAheadLogBasedBlockHandler 是同时写 WAL(4c) 和 executor 的内存或硬盘 (5) 每次成块在 executor 存储完毕后，ReceiverSupervisor 就会及时上报块数据的 meta 信息给 driver 端的 ReceiverTracker；这里的 meta 信息包括数据的标识 id，数据的位置，数据的条数，数据的大小等信息。 (6) ReceiverTracker 再将收到的块数据 meta 信息直接转给自己的成员 ReceivedBlockTracker，由 ReceivedBlockTracker 专门管理收到的块数据 meta 信息。 BlockGenerator 的全限定名是：org.apache.spark.streaming.receiver.BlockGenerator BlockManagerBasedBlockHandler 的全限定名是：org.apache.spark.streaming.receiver.BlockManagerBasedBlockHandler WriteAheadLogBasedBlockHandler 的全限定名是：org.apache.spark.streaming.receiver.WriteAheadLogBasedBlockHandler ReceivedBlockTracker 的全限定名是：org.apache.spark.streaming.scheduler.ReceivedBlockTracker ReceiverInputDStream 的全限定名是：org.apache.spark.streaming.dstream.ReceiverInputDStream 这里 (3)(4)(5)(6) 的过程是一直持续不断地发生的，我们也将其在上图里标识出来。 后续在 driver 端，就由 ReceiverInputDStream 在每个 batch 去检查 ReceiverTracker 收到的块数据 meta 信息，界定哪些新数据需要在本 batch 内处理，然后生成相应的 RDD 实例去处理这些块数据。 下面我们来详解 Receiver, ReceiverSupervisor, BlockGenerator 这三个类。 Receiver 详解Receiver 是一个 abstract 的基类： 123456789101112131415// 来自 Receiverabstract class Receiver[T](val storageLevel: StorageLevel) extends Serializable &#123; // 需要子类实现 def onStart() def onStop() // 基类实现，供子类调用 def store(dataItem: T) &#123;...&#125; // 【存储单条小数据】 def store(dataBuffer: ArrayBuffer[T]) &#123;...&#125; // 【存储数组形式的块数据】 def store(dataIterator: Iterator[T]) &#123;...&#125; // 【存储 iterator 形式的块数据】 def store(bytes: ByteBuffer) &#123;...&#125; // 【存储 ByteBuffer 形式的块数据】 ...&#125; 这里需要 Receiver 子类具体实现的是，onStart() 和 onStop() 方法。onStart() 是在 executor 端被 ReceiverSupervisor 调用的，而且 onStart() 的实现应该很快就能返回，不要写成阻塞式的。 比如，Spark Streaming 自带的 SocketReceiver 的 onStart() 实现如下： 123456789// 来自 SocketReceiverdef onStart() &#123; new Thread(\"Socket Receiver\") &#123; setDaemon(true) override def run() &#123; receive() &#125; &#125;.start() // 【仅新拉起了一个线程来接收数据】 // 【onStart() 方法很快就返回了】&#125; 另外的 onStop() 实现，就是在 Receiver 被关闭时调用了，可以做一些 close 工作。 我们看当 Receiver 真正启动起来后，可以开始产生或者接收数据了，那接收到的数据该怎么存到 Spark Streaming 里？ 答案很简单，就是直接调用 store() 方法即可。Receiver 基类提供了 4 种签名的 store() 方法，分别可用于存储： (a) 单条小数据 (b) 数组形式的块数据 (c) iterator 形式的块数据 (d) ByteBuffer 形式的块数据 这 4 种签名的 store() 的实现都是直接将数据转给 ReceiverSupervisor，由 ReceiverSupervisor 来具体负责存储。 所以，一个具体的 Receiver 子类实现，只要在 onStart() 里新拉起数据接收线程，并在接收到数据时 store() 到 Spark Streamimg 框架就可以了。 ReceiverSupervisor 详解我们在 Receiver 分发详解 里分析过，在 executor 端，分发 Receiver 的 Job 的 Task 执行的实现是： 12345678910(iterator: Iterator[Receiver[_]]) =&gt; &#123; ... val receiver = iterator.next() assert(iterator.hasNext == false) // 【ReceiverSupervisor 的具体实现 ReceiverSupervisorImpl】 val supervisor = new ReceiverSupervisorImpl(receiver, ...) supervisor.start() supervisor.awaitTermination() ...&#125; ReceiverSupervisor 定义了一些方法接口，其具体的实现类是 ReceiverSupervisorImpl。 我们看到在上面的代码中，executor 端会先 new 一个 ReceiverSupervisorImpl，然后 ReceiverSupervisorImpl.start()。这里 .start() 很重要的工作就是调用 Receiver.onStart()，来启动 Receiver 的数据接收线程： start() 成功后，ReceiverSurpervisorImpl 最重要的工作就是接收 Receiver 给 store() 过来的数据了。 ReceiverSurpervisorImpl 有 4 种签名的 push() 方法，被 Receiver 的 4 种 store() 一一调用。不过接下来对单条小数据和三种块数据的处理稍有区别。 单条的情况，ReceiverSupervisorImpl 要在 BlockGenerator 的协助下，将多个单条的数据积攒为一个块数据，然后重新调用 push 交给 ReceiverSurpervisorImpl 来处理这个块数据。我们一会再详解 BlockGenerator 的这个过程。 所以接下来，我们主要看这 3 个存储块数据的 push...() 方法，它们的实现非常简单： 1234567891011121314151617// 来自 ReceiverSupervisorImpldef pushArrayBuffer(arrayBuffer: ArrayBuffer[_], ...) &#123; pushAndReportBlock(ArrayBufferBlock(...), ...)&#125;def pushIterator(iterator: Iterator[_], ...) &#123; pushAndReportBlock(IteratorBlock(...), ...)&#125;def pushBytes(bytes: ByteBuffer, ...)&#123; pushAndReportBlock(ByteBufferBlock(...), ...)&#125;def pushAndReportBlock(receivedBlock: ReceivedBlock, ...) &#123;...&#125; 顾名思义，这 3 个存储块数据的 push...() 方法即是将自己的数据统一包装为 ReceivedBlock，然后由 pushAndReportBlock() 做两件事情： (a) push：将 ReceivedBlock 交给 ReceivedBlockHandler 来存储，具体的，可以在 ReceivedBlockHandler 的两种存储实现里二选一 (b) report：将已存储好的 ReceivedBlock 的块数据 meta 信息报告给 ReceiverTracker 上面的过程可以总结为： ReceivedBlockHandler 详解ReceivedBlockHandler 是一个接口类，在 executor 端负责对接收到的块数据进行具体的存储和清理： 12345678910// 来自 ReceivedBlockHandlerprivate[streaming] trait ReceivedBlockHandler &#123; /** Store a received block with the given block id and return related metadata */ def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): ReceivedBlockStoreResult /** Cleanup old blocks older than the given threshold time */ def cleanupOldBlocks(threshTime: Long)&#125; ReceivedBlockHandler 有两个具体的存储策略的实现： (a) BlockManagerBasedBlockHandler，是直接存到 executor 的内存或硬盘 (b) WriteAheadLogBasedBlockHandler，是先写 WAL，再存储到 executor 的内存或硬盘 (a) BlockManagerBasedBlockHandler 实现BlockManagerBasedBlockHandler 主要是直接存储到 Spark Core 里的 BlockManager 里。 BlockManager 将在 executor 端接收 Block 数据，而在 driver 端维护 Block 的 meta 信息。 BlockManager 根据存储者的 StorageLevel 要求来存到本 executor 的 RAM 或者 DISK，也可以同时再额外复制一份到其它 executor 的 RAM 或者 DISK。点这里查看 StorageLevel 支持的所有枚举值。 下面是 BlockManagerBasedBlockHandler.store() 向 BlockManager 存储 3 种块数据的具体实现：123456789101112// 来自 BlockManagerBasedBlockHandlerdef storeBlock(blockId: StreamBlockId, block: ReceivedBlock): ReceivedBlockStoreResult = &#123; val putResult: Seq[(BlockId, BlockStatus)] = block match &#123; case ArrayBufferBlock(arrayBuffer) =&gt; blockManager.putIterator(blockId, arrayBuffer.iterator, ...) // 【存储数组到 blockManager 里】 case IteratorBlock(iterator) =&gt; blockManager.putIterator(blockId, countIterator, ...) // 【存储 iterator 到 blockManager 里】 case ByteBufferBlock(byteBuffer) =&gt; blockManager.putBytes(blockId, byteBuffer, ...) // 【存储 ByteBuffer 到 blockManager 里】 ...&#125; (b) WriteAheadLogBasedBlockHandler 实现WriteAheadLogBasedBlockHandler 的实现则是同时写到可靠存储的 WAL 中和 executor 的 BlockManager 中；在两者都写完成后，再上报块数据的 meta 信息。 BlockManager 中的块数据是计算时首选使用的，只有在 executor 失效时，才去 WAL 中读取写入过的数据。 同其它系统的 WAL 一样，数据是完全顺序地写入 WAL 的；在稍后上报块数据的 meta 信息，就额外包含了块数据所在的 WAL 的路径，及在 WAL 文件内的偏移地址和长度。 具体的写入逻辑如下： 1234567891011121314151617181920212223242526// 来自 WriteAheadLogBasedBlockHandlerdef storeBlock(blockId: StreamBlockId, block: ReceivedBlock): ReceivedBlockStoreResult = &#123; ... // 【生成向 BlockManager 存储数据的 future】 val storeInBlockManagerFuture = Future &#123; val putResult = blockManager.putBytes(blockId, serializedBlock, effectiveStorageLevel, tellMaster = true) if (!putResult.map &#123; _._1 &#125;.contains(blockId)) &#123; throw new SparkException( s\"Could not store $blockId to block manager with storage level $storageLevel\") &#125; &#125; // 【生成向 WAL 存储数据的 future】 val storeInWriteAheadLogFuture = Future &#123; writeAheadLog.write(serializedBlock, clock.getTimeMillis()) &#125; // 【开始执行两个 future、等待两个 future 都结束】 val combinedFuture = storeInBlockManagerFuture.zip(storeInWriteAheadLogFuture).map(_._2) val walRecordHandle = Await.result(combinedFuture, blockStoreTimeout) // 【返回存储结果，用于后续的块数据 meta 上报】 WriteAheadLogBasedStoreResult(blockId, numRecords, walRecordHandle)&#125; BlockGenerator 详解最后我们来补充一下 ReceiverSupervisorImpl 在收到单块条小数据后，委托 BlockGenerator 进行积攒，并封装多条小数据为一整个块数据的详细过程。 BlockGenerator 在内部主要是维护一个临时的变长数组 currentBuffer，每收到一条 ReceiverSupervisorImpl 转发来的数据就加入到这个 currentBuffer 数组中。 这里非常需要注意的地方，就是在加入 currentBuffer 数组时会先由 rateLimiter 检查一下速率，是否加入的频率已经太高。如果太高的话，就需要 block 住，等到下一秒再开始添加。这里的最高频率是由 spark.streaming.receiver.maxRate (default = Long.MaxValue) 控制的，是单个 Receiver 每秒钟允许添加的条数。控制了这个速率，就控制了整个 Spark Streaming 系统每个 batch 需要处理的最大数据量。之前版本的 Spark Streaming 是静态设置了这样的一个上限并由所有 Receiver 统一遵守；但在 1.5.0 以来，Spark Streaming 加入了分别动态控制每个 Receiver 速率的特性，这个我们会单独有一篇文章介绍。 然后会维护一个定时器，每隔 blockInterval 的时间就生成一个新的空变长数组替换老的数组作为新的 currentBuffer ，并把老的数组加入到一个自己的一个 blocksForPushing 的队列里。 这个 blocksForPushing 队列实际上是一个 ArrayBlockingQueue，大小由 spark.streaming.blockQueueSize（默认 = 10） 来控制。然后就有另外的一个线程专门从这个队列里取出来已经包装好的块数据，然后调用 ReceiverSupervisorImpl.pushArrayBuffer(...) 来将块数据交回给 ReceiverSupervisorImpl。 BlockGenerator 工作的整个过程示意图如下： //TODO(lwlin): 此图风格与本系列文章不符，需要美化 总结总结我们在本文所做的详解 —— ReceiverSupervisor 将在 executor 端作为的主要角色，并且： (3) Receiver 在 onStart() 启动后，就将持续不断地接收外界数据，并持续交给 ReceiverSupervisor 进行数据转储； (4) ReceiverSupervisor 持续不断地接收到 Receiver 转来的数据： 如果数据很细小，就需要 BlockGenerator 攒多条数据成一块(4a)、然后再成块存储(4b 或 4c) 反之就不用攒，直接成块存储(4b 或 4c) 这里 Spark Streaming 目前支持两种成块存储方式，一种是由 blockManagerskManagerBasedBlockHandler 直接存到 executor 的内存或硬盘，另一种由 WriteAheadLogBasedBlockHandler 是同时写 WAL(4c) 和 executor 的内存或硬盘 (5) 每次成块在 executor 存储完毕后，ReceiverSupervisor 就会及时上报块数据的 meta 信息给 driver 端的 ReceiverTracker；这里的 meta 信息包括数据的标识 id，数据的位置，数据的条数，数据的大小等信息。 (6) ReceiverTracker 再将收到的块数据 meta 信息直接转给自己的成员 ReceivedBlockTracker，由 ReceivedBlockTracker 专门管理收到的块数据 meta 信息。 （本文完，参与本文的讨论请 猛戳这里，返回目录请 猛戳这里）","categories":[],"tags":[]},{"title":"","slug":"bigdata/spark/Spark Streaming 源码解析系列/3.1 Receiver 分发详解","date":"2018-02-09T03:49:13.914Z","updated":"2018-02-09T03:48:41.427Z","comments":true,"path":"2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/3.1 Receiver 分发详解/","link":"","permalink":"http://yoursite.com/2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/3.1 Receiver 分发详解/","excerpt":"","text":"Receiver 分发详解[酷玩 Spark] Spark Streaming 源码解析系列 ，返回目录请 猛戳这里 「腾讯·广点通」技术团队荣誉出品 12345本系列内容适用范围：* 2017.07.11 update, Spark 2.2 全系列 √ (已发布：2.2.0)* 2017.10.02 update, Spark 2.1 全系列 √ (已发布：2.1.0, 2.1.1, 2.1.2)* 2016.11.14 update, Spark 2.0 全系列 √ (已发布：2.0.0, 2.0.1, 2.0.2) 阅读本文前，请一定先阅读 Spark Streaming 实现思路与模块概述 一文，其中概述了 Spark Streaming 的 4 大模块的基本作用，有了全局概念后再看本文对 模块 3：数据产生与导入 细节的解释。 引言我们前面在 DStream, DStreamGraph 详解 讲到，整个 DStreamGraph 是由 output stream 通过 dependency 引用关系，索引到上游 DStream 节点。而递归的追溯到最上游的 InputDStream 节点时，就没有对其它 DStream 节点的依赖了，因为 InputDStream 节点本身就代表了最原始的数据集。 我们对 模块 3：数据产生与导入 细节的解释，是仅针对 ReceiverInputDStream 及其子类的；其它 InputDStream 子类的讲解，我们在另外的文章中进行。即，本模块的讨论范围是： 123456789- ReceiverInputDStream - 子类 SocketInputDStream - 子类 TwitterInputDStream - 子类 RawInputDStream - 子类 FlumePollingInputDStream - 子类 MQTTInputDStream - 子类 FlumeInputDStream - 子类 PluggableInputDStream - 子类 KafkaInputDStream ReceiverTracker 分发 Receiver 过程我们已经知道，ReceiverTracker 自身运行在 driver 端，是一个管理分布在各个 executor 上的 Receiver 的总指挥者。 在 ssc.start() 时，将隐含地调用 ReceiverTracker.start()；而 ReceiverTracker.start() 最重要的任务就是调用自己的 launchReceivers() 方法将 Receiver 分发到多个 executor 上去。然后在每个 executor 上，由 ReceiverSupervisor 来分别启动一个 Receiver 接收数据。这个过程用下图表示： 我们将以 1.4.0 和 1.5.0 这两个版本为代表，仔细分析一下 launchReceivers() 的实现。 1.4.0 代表了 1.5.0 以前的版本，如 1.2.x, 1.3.x, 1.4.x 1.5.0 代表了 1.5.0 以来的版本，如 1.5.x, 1.6.x Spark 1.4.0 的 launchReceivers() 实现Spark 1.4.0 的 launchReceivers() 的过程如下： (1.a) 构造 Receiver RDD。具体的，是先遍历所有的 ReceiverInputStream，获得将要启动的所有 x 个 Receiver 的实例。然后，把这些实例当做 x 份数据，在 driver 端构造一个 RDD 实例，这个 RDD 分为 x 个 partition，每个 partition 包含一个 Receiver 数据（即 Receiver 实例）。 (1.b) 定义计算 func。我们将在多个 executor 上共启动 x 个 Task，每个 Task 负责一个 partition 的数据，即一个 Receiver 实例。我们要对这个 Receiver 实例做的计算定义为 func 函数，具体的，func 是： 以这个 Receiver 实例为参数，构造新的 ReceiverSupervisor 实例 supervisor：supervisor = new ReceiverSupervisorImpl(receiver, ...) supervisor.start()；这一步将启动新线程启动 Receiver 实例，然后很快返回 supervisor.awaitTermination()；将一直 block 住当前 Task 的线程 (1.c) 分发 RDD(Receiver) 和 func 到具体的 executor。上面 (a)(b) 两步只是在 driver 端定义了 RDD[Receiver] 和 这个 RDD 之上将执行的 func，但并没有具体的去做。这一步是将两者的定义分发到 executor 上去，马上就可以实际执行了。 (2) 在各个 executor 端，执行(1.b) 中定义的 func。即启动 Receiver 实例，并一直 block 住当前线程。 这样，通过 1 个 RDD 实例包含 x 个 Receiver，对应启动 1 个 Job 包含 x 个 Task，就可以完成 Receiver 的分发和部署了。上述 (1.a)(1.b)(1.c)(2) 的过程示意如下图： 这里 Spark Streaming 下层的 Spark Core 对 Receiver 分发是毫无感知的，它只是执行了“应用层面” – 对 Spark Core 来讲，Spark Streaming 就是“应用层面”– 的一个普通 Job；但 Spark Streaming 只通过这个普通 Job 即可完“特殊功能”的 Receiver 分发，可谓巧妙巧妙。 上述逻辑实现的源码请到 Spark 1.4.0 的 ReceiverTracker 查看。 Spark 1.5.0 的 launchReceivers() 实现其实上面这个实现，这个长时运行的分发 Job 还存在一些问题： 如果某个 Task 失败超过 spark.task.maxFailures(默认=4) 次的话，整个 Job 就会失败。这个在长时运行的 Spark Streaming 程序里，Executor 多失效几次就有可能导致 Task 失败达到上限次数了。 如果某个 Task 失效一下，Spark Core 的 TaskScheduler 会将其重新部署到另一个 executor 上去重跑。但这里的问题在于，负责重跑的 executor 可能是在下发重跑的那一刻是正在执行 Task 数较少的，但不一定能够将 Receiver 分布的最均衡的。 有个用户 code 可能会想自定义一个 Receiver 的分布策略，比如所有的 Receiver 都部署到同一个节点上去。 从 1.5.0 开始，Spark Streaming 添加了增强的 Receiver 分发策略。对比之前的版本，主要的变更在于： 添加可插拔的 ReceiverSchedulingPolicy 把 1 个 Job（包含 x 个 Task），改为 x 个 Job（每个 Job 只包含 1 个 Task） 添加对 Receiver 的监控重启机制 我们一个一个看一看。 (1) 可插拔的 ReceiverSchedulingPolicyReceiverSchedulingPolicy 的主要目的，是在 Spark Streaming 层面添加对 Receiver 的分发目的地的计算，相对于之前版本依赖 Spark Core 的 TaskScheduler 进行通用分发，新的 ReceiverSchedulingPolicy 会对 Streaming 应用的更好的语义理解，也能计算出更好的分发策略。 ReceiverSchedulingPolicy 有两个方法，分别用于： 在 Streaming 程序首次启动时： 收集所有 InputDStream 包含的所有 Receiver 实例 —— receivers 收集所有的 executor —— executors —— 作为候选目的地 然后就调用 ReceiverSchedulingPolicy.scheduleReceivers(receivers, executors) 来计算每个 Receiver 的目的地 executor 列表 在 Streaming 程序运行过程中，如果需要重启某个 Receiver： 将首先看一看之前计算过的目的地 executor 有没有还 alive 的 如果没有，就需要 ReceiverSchedulingPolicy.rescheduleReceiver(receiver, ...) 来重新计算这个 Receiver 的目的地 executor 列表 默认的 ReceiverSchedulingPolicy 是实现为 round-robin 式的了。我们举例说明下这两个方法： 其中，在 Receiver y 失效时，以前的 Spark Streaming 有可能会在 executor 1 上重启 Receiver y，而 1.5.0 以来，将在 executor 3 上重启 Receiver y。 (2) 每个 Receiver 分发有单独的 Job 负责1.5.0 版本以来的 Spark Streaming，是为每个 Receiver 都分配单独的只有 1 个 Task 的 Job 来尝试分发，这与以前版本将 x 个 Receiver 都放到一个有 x 个 Task 的 Job 里分发是很不一样的。 而且，对于这仅有的一个 Task，只在第 1 次执行时，才尝试启动 Receiver；如果该 Task 因为失效而被调度到其它 executor 执行时，就不再尝试启动 Receiver、只做一个空操作，从而导致本 Job 的状态是成功执行已完成。ReceiverTracker 会另外调起一个 Job —— 有可能会重新计算 Receiver 的目的地 —— 来继续尝试 Receiver 分发……如此直到成功为止。 另外，由于 Spark Core 的 Task 下发时只会参考并大部分时候尊重 Spark Streaming 设置的 preferredLocation 目的地信息，还是有一定可能该分发 Receiver 的 Job 并没有在我们想要调度的 executor 上运行。此时，在第 1 次执行 Task 时，会首先向 ReceiverTracker 发送 RegisterReceiver 消息，只有得到肯定的答复时，才真正启动 Receiver，否则就继续做一个空操作，导致本 Job 的状态是成功执行已完成。当然，ReceiverTracker 也会另外调起一个 Job，来继续尝试 Receiver 分发……如此直到成功为止。 我们用图示来表达这个改动： 所以通过上面可以看到，一个 Receiver 的分发 Job 是有可能没有完成分发 Receiver 的目的的，所以 ReceiverTracker 会继续再起一个 Job 来尝试 Receiver 分发。这个机制保证了，如果一次 Receiver 如果没有抵达预先计算好的 executor，就有机会再次进行分发，从而实现在 Spark Streaming 层面对 Receiver 所在位置更好的控制。 (3) 对 Receiver 的监控重启机制上面分析了每个 Receiver 都有专门的 Job 来保证分发后，我们发现这样一来，Receiver 的失效重启就不受 spark.task.maxFailures(默认=4) 次的限制了。 因为现在的 Receiver 重试不是在 Task 级别，而是在 Job 级别；并且 Receiver 失效后并不会导致前一次 Job 失败，而是前一次 Job 成功、并新起一个 Job 再次进行分发。这样一来，不管 Spark Streaming 运行多长时间，Receiver 总是保持活性的，不会随着 executor 的丢失而导致 Receiver 死去。 总结我们再简单对比一下 1.4.0 和 1.5.0 版本在 Receiver 分发上的区别： 通过以上分析，我们总结： Spark Streaming 1.4.0 Spark Streaming 1.5.0 Receiver 活性 不保证永活 无限重试、保证永活 Receiver 均衡分发 无保证 round-robin 策略 自定义 Receiver 分发 很 tricky 方便 致谢本文所分析的 1.5.0 以来增强的 Receiver 分发策略，是由朱诗雄同学强势贡献给社区的： 朱诗雄，Apache Spark Committer, Databricks 的中国籍牛牛工程师，已为 Spark 持续贡献代码近两年时间 强势围观 他的 Github，和 他正为 Spark 贡献的代码 （本文完，参与本文的讨论请 猛戳这里，返回目录请 猛戳这里）","categories":[],"tags":[]},{"title":"","slug":"bigdata/spark/Spark Streaming 源码解析系列/2.2 JobGenerator 详解","date":"2018-02-09T03:49:13.906Z","updated":"2018-02-09T03:48:41.424Z","comments":true,"path":"2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/2.2 JobGenerator 详解/","link":"","permalink":"http://yoursite.com/2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/2.2 JobGenerator 详解/","excerpt":"","text":"JobGenerator 详解[酷玩 Spark] Spark Streaming 源码解析系列 ，返回目录请 猛戳这里 「腾讯·广点通」技术团队荣誉出品 12345本系列内容适用范围：* 2017.07.11 update, Spark 2.2 全系列 √ (已发布：2.2.0)* 2017.10.02 update, Spark 2.1 全系列 √ (已发布：2.1.0, 2.1.1, 2.1.2)* 2016.11.14 update, Spark 2.0 全系列 √ (已发布：2.0.0, 2.0.1, 2.0.2) 阅读本文前，请一定先阅读 Spark Streaming 实现思路与模块概述 一文，其中概述了 Spark Streaming 的 4 大模块的基本作用，有了全局概念后再看本文对 模块 2：Job 动态生成 细节的解释。 引言前面在 Spark Streaming 实现思路与模块概述 和 DStream 生成 RDD 实例详解 里我们分析了 DStreamGraph 和 DStream 具有能够实例化 RDD 和 RDD DAG 的能力，下面我们来看 Spark Streaming 是如何将其动态调度的。 在 Spark Streaming 程序的入口，我们都会定义一个 batchDuration，就是需要每隔多长时间就比照静态的 DStreamGraph 来动态生成一个 RDD DAG 实例。在 Spark Streaming 里，总体负责动态作业调度的具体类是 JobScheduler， JobScheduler 有两个非常重要的成员：JobGenerator 和 ReceiverTracker。JobScheduler 将每个 batch 的 RDD DAG 具体生成工作委托给 JobGenerator，而将源头输入数据的记录工作委托给 ReceiverTracker。 JobScheduler 的全限定名是：org.apache.spark.streaming.scheduler.JobScheduler JobGenerator 的全限定名是：org.apache.spark.streaming.scheduler.JobGenerator ReceiverTracker 的全限定名是：org.apache.spark.streaming.scheduler.ReceiverTracker 本文我们来详解 JobScheduler。 JobGenerator 启动在用户 code 最后调用 ssc.start() 时，将隐含的导致一系列模块的启动，其中对我们 JobGenerator 这里的启动调用关系如下：12345// 来自 StreamingContext.start(), JobScheduler.start(), JobGenerator.start()ssc.start() // 【用户 code：StreamingContext.start()】 -&gt; scheduler.start() // 【JobScheduler.start()】 -&gt; jobGenerator.start() // 【JobGenerator.start()】 具体的看，JobGenerator.start() 的代码如下： 123456789101112// 来自 JobGenerator.start()def start(): Unit = synchronized &#123; ... eventLoop.start() // 【启动 RPC 处理线程】 if (ssc.isCheckpointPresent) &#123; restart() // 【如果不是第一次启动，就需要从 checkpoint 恢复】 &#125; else &#123; startFirstTime() // 【第一次启动，就 startFirstTime()】 &#125;&#125; 可以看到，在启动了 RPC 处理线程 eventLoop 后，就会根据是否是第一次启动，也就是是否存在 checkpoint，来具体的决定是 restart() 还是 startFirstTime()。 后面我们会分析失效后重启的 restart() 流程，这里我们来关注 startFirstTime(): 12345678// 来自 JobGenerator.startFirstTime()private def startFirstTime() &#123; val startTime = new Time(timer.getStartTime()) graph.start(startTime - graph.batchDuration) timer.start(startTime.milliseconds) logInfo(\"Started JobGenerator at \" + startTime)&#125; 可以看到，这里首次启动时做的工作，先是通过 graph.start() 来告知了 DStreamGraph 第 1 个 batch 的启动时间，然后就是 timer.start() 启动了关键的定时器。 当定时器 timer 启动以后，JobGenerator 的 startFirstTime() 就完成了。 RecurringTimer通过之前几篇文章的分析我们知道，JobGenerator 维护了一个定时器，周期就是用户设置的 batchDuration，定时为每个 batch 生成 RDD DAG 的实例。 具体的，这个定时器实例就是：123456789// 来自 JobGeneratorprivate[streaming]class JobGenerator(jobScheduler: JobScheduler) extends Logging &#123;... private val timer = new RecurringTimer(clock, ssc.graph.batchDuration.milliseconds, longTime =&gt; eventLoop.post(GenerateJobs(new Time(longTime))), \"JobGenerator\")...&#125; 通过代码也可以看到，整个 timer 的调度周期就是 batchDuration，每次调度起来就是做一个非常简单的工作：往 eventLoop 里发送一个消息 —— 该为当前 batch (new Time(longTime)) GenerateJobs 了！ GenerateJobs接下来，eventLoop 收到消息时，会在一个消息处理的线程池里，执行对应的操作。在这里，处理 GenerateJobs(time) 消息的对应操作是 generateJobs(time)： 1234567891011121314private def generateJobs(time: Time) &#123; SparkEnv.set(ssc.env) Try &#123; jobScheduler.receiverTracker.allocateBlocksToBatch(time) // 【步骤 (1)】 graph.generateJobs(time) // 【步骤 (2)】 &#125; match &#123; case Success(jobs) =&gt; val streamIdToInputInfos = jobScheduler.inputInfoTracker.getInfo(time) // 【步骤 (3)】 jobScheduler.submitJobSet(JobSet(time, jobs, streamIdToInputInfos)) // 【步骤 (4)】 case Failure(e) =&gt; jobScheduler.reportError(\"Error generating jobs for time \" + time, e) &#125; eventLoop.post(DoCheckpoint(time, clearCheckpointDataLater = false)) // 【步骤 (5)】&#125; 这段代码异常精悍，包含了 JobGenerator 主要工作 —— 如下图所示 —— 的 5 个步骤！ (1) 要求 ReceiverTracker 将目前已收到的数据进行一次 allocate，即将上次 batch 切分后的数据切分到到本次新的 batch 里 这里 ReceiverTracker 对已收到数据的 meta 信息进行 allocateBlocksToBatch(time)，与 ReceiverTracker 自己接收 ReceiverSupervisorImpl 上报块数据 meta 信息的过程，是相互独立的，但通过 synchronized 关键字来互斥同步 即是说，不管 ReceiverSupervisorImpl 形成块数据的时间戳 t1、ReceiverSupervisorImpl 发送块数据的时间戳 t2、ReceiverTracker 收到块数据的时间戳 t3 分别是啥，最终块数据划入哪个 batch，还是由 ReceiverTracker.allocateBlocksToBatch(time) 方法获得 synchronized 锁的那一刻，还有未划入之前任何一个 batch 的块数据 meta，将被划分入最新的 batch 所以，每个块数据的 meta 信息，将被划入一个、且只被划入一个 batch (2) 要求 DStreamGraph 复制出一套新的 RDD DAG 的实例，具体过程是：DStreamGraph 将要求图里的尾 DStream 节点生成具体的 RDD 实例，并递归的调用尾 DStream 的上游 DStream 节点……以此遍历整个 DStreamGraph，遍历结束也就正好生成了 RDD DAG 的实例 这个过程的详解，请参考前面的文章 DStream 生成 RDD 实例详解 精确的说，整个 DStreamGraph.generateJobs(time) 遍历结束的返回值是 Seq[Job] (3) 获取第 1 步 ReceiverTracker 分配到本 batch 的源头数据的 meta 信息 第 1 步中 ReceiverTracker 只是对 batch 的源头数据 meta 信息进行了 batch 的分配，本步骤是按照 batch 时间来向 ReceiverTracker 查询得到划分到本 batch 的块数据 meta 信息 (4) 将第 2 步生成的本 batch 的 RDD DAG，和第 3 步获取到的 meta 信息，一同提交给 JobScheduler 异步执行 这里我们提交的是将 (a) time (b) Seq[job] (c) 块数据的 meta 信息 这三者包装为一个 JobSet，然后调用 JobScheduler.submitJobSet(JobSet) 提交给 JobScheduler 这里的向 JobScheduler 提交过程与 JobScheduler 接下来在 jobExecutor 里执行过程是异步分离的，因此本步将非常快即可返回 (5) 只要提交结束（不管是否已开始异步执行），就马上对整个系统的当前运行状态做一个 checkpoint 这里做 checkpoint 也只是异步提交一个 DoCheckpoint 消息请求，不用等 checkpoint 真正写完成即可返回 这里也简单描述一下 checkpoint 包含的内容，包括已经提交了的、但尚未运行结束的 JobSet 等实际运行时信息。 （本文完，参与本文的讨论请 猛戳这里，返回目录请 猛戳这里）","categories":[],"tags":[]},{"title":"","slug":"bigdata/spark/Spark Streaming 源码解析系列/2.1 JobScheduler, Job, JobSet 详解","date":"2018-02-09T03:49:13.899Z","updated":"2018-02-09T03:48:41.423Z","comments":true,"path":"2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/2.1 JobScheduler, Job, JobSet 详解/","link":"","permalink":"http://yoursite.com/2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/2.1 JobScheduler, Job, JobSet 详解/","excerpt":"","text":"JobScheduler, Job, JobSet 详解[酷玩 Spark] Spark Streaming 源码解析系列 ，返回目录请 猛戳这里 「腾讯·广点通」技术团队荣誉出品 12345本系列内容适用范围：* 2017.07.11 update, Spark 2.2 全系列 √ (已发布：2.2.0)* 2017.10.02 update, Spark 2.1 全系列 √ (已发布：2.1.0, 2.1.1, 2.1.2)* 2016.11.14 update, Spark 2.0 全系列 √ (已发布：2.0.0, 2.0.1, 2.0.2) 阅读本文前，请一定先阅读 Spark Streaming 实现思路与模块概述 一文，其中概述了 Spark Streaming 的 4 大模块的基本作用，有了全局概念后再看本文对 模块 2：Job 动态生成 细节的解释。 引言前面在 Spark Streaming 实现思路与模块概述 和 DStream 生成 RDD 实例详解 里我们分析了 DStreamGraph 和 DStream 具有能够实例化 RDD 和 RDD DAG 的能力，下面我们来看 Spark Streaming 是如何将其动态调度的。 在 Spark Streaming 程序的入口，我们都会定义一个 batchDuration，就是需要每隔多长时间就比照静态的 DStreamGraph 来动态生成一个 RDD DAG 实例。在 Spark Streaming 里，总体负责动态作业调度的具体类是 JobScheduler，在 Spark Streaming 程序在 ssc.start() 开始运行时，将 JobScheduler 的实例给 start() 运行起来。 123456789101112// 来自 StreamingContextdef start(): Unit = synchronized &#123; ... ThreadUtils.runInNewThread(\"streaming-start\") &#123; sparkContext.setCallSite(startSite.get) sparkContext.clearJobGroup() sparkContext.setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, \"false\") scheduler.start() // 【这里调用了 JobScheduler().start()】 &#125; state = StreamingContextState.ACTIVE ...&#125; Spark Streaming 的 Job 总调度者 JobSchedulerJobScheduler 是 Spark Streaming 的 Job 总调度者。 JobScheduler 有两个非常重要的成员：JobGenerator 和 ReceiverTracker。JobScheduler 将每个 batch 的 RDD DAG 具体生成工作委托给 JobGenerator，而将源头输入数据的记录工作委托给 ReceiverTracker。 JobScheduler 的全限定名是：org.apache.spark.streaming.scheduler.JobScheduler JobGenerator 的全限定名是：org.apache.spark.streaming.scheduler.JobGenerator ReceiverTracker 的全限定名是：org.apache.spark.streaming.scheduler.ReceiverTracker JobGenerator 维护了一个定时器，周期就是我们刚刚提到的 batchDuration，定时为每个 batch 生成 RDD DAG 的实例。具体的，根据我们在 DStream 生成 RDD 实例详解 中的解析，DStreamGraph.generateJobs(time) 将返回一个 Seq[Job]，其中的每个 Job 是一个 ForEachDStream 实例的 generateJob(time) 返回的结果。 此时，JobGenerator 拿到了 Seq[Job] 后（如上图 (2) ），就将其包装成一个 JobSet（如上图 (3) ），然后就调用 JobScheduler.submitJobSet(jobSet) 来交付回 JobScheduler（如上图 (4) ）。 那么 JobScheduler 收到 jobSet 后是具体如何处理的呢？我们看其实现：12345678910// 来自 JobScheduler.submitJobSet(jobSet: JobSet)if (jobSet.jobs.isEmpty) &#123; logInfo(\"No jobs added for time \" + jobSet.time)&#125; else &#123; listenerBus.post(StreamingListenerBatchSubmitted(jobSet.toBatchInfo)) jobSets.put(jobSet.time, jobSet) // 【下面这行是最主要的处理逻辑：将每个 job 都在 jobExecutor 线程池中、用 new JobHandler 来处理】 jobSet.jobs.foreach(job =&gt; jobExecutor.execute(new JobHandler(job))) logInfo(\"Added jobs for time \" + jobSet.time)&#125; 这里最重要的处理逻辑是 job =&gt; jobExecutor.execute(new JobHandler(job))，也就是将每个 job 都在 jobExecutor 线程池中、用 new JobHandler 来处理。 JobHandler先来看 JobHandler 针对 Job 的主要处理逻辑：1234567891011121314151617// 来自 JobHandlerdef run()&#123; ... // 【发布 JobStarted 消息】 _eventLoop.post(JobStarted(job)) PairRDDFunctions.disableOutputSpecValidation.withValue(true) &#123; // 【主要逻辑，直接调用了 job.run()】 job.run() &#125; _eventLoop = eventLoop if (_eventLoop != null) &#123; // 【发布 JobCompleted 消息】 _eventLoop.post(JobCompleted(job)) &#125; ...&#125; 也就是说，JobHandler 除了做一些状态记录外，最主要的就是调用 job.run()！这里就与我们在 DStream 生成 RDD 实例详解 里分析的对应起来了：在 ForEachDStream.generateJob(time) 时，是定义了 Job 的运行逻辑，即定义了 Job.func。而在 JobHandler 这里，是真正调用了 Job.run()、将触发 Job.func 的真正执行！ Job 运行的线程池 jobExecutor上面 JobHandler 是解决了做什么的问题，本节 jobExecutor 是解决 Job 在哪里做。 具体的，jobExecutor 是 JobScheduler 的成员： 123456789// 来自 JobSchedulerprivate[streaming]class JobScheduler(val ssc: StreamingContext) extends Logging &#123; ... private val numConcurrentJobs = ssc.conf.getInt(\"spark.streaming.concurrentJobs\", 1) private val jobExecutor = ThreadUtils.newDaemonFixedThreadPool(numConcurrentJobs, \"streaming-job-executor\") ...&#125; 也就是，ThreadUtils.newDaemonFixedThreadPool() 调用将产生一个名为 &quot;streaming-job-executor&quot; 的线程池，所以，Job 将在这个线程池的线程里，被实际执行 func。 spark.streaming.concurrentJobs 参数这里 jobExecutor 的线程池大小，是由 spark.streaming.concurrentJobs 参数来控制的，当没有显式设置时，其取值为 1。 进一步说，这里 jobExecutor 的线程池大小，就是能够并行执行的 Job 数。而回想前文讲解的 DStreamGraph.generateJobs(time) 过程，一次 batch 产生一个 Seq[Job}，里面可能包含多个 Job —— 所以，确切的，有几个 output 操作，就调用几次 ForEachDStream.generatorJob(time)，就产生出几个 Job 。 为了验证这个结果，我们做一个简单的小测试：先设置 spark.streaming.concurrentJobs = 10，然后在每个 batch 里做 2 次 foreachRDD() 这样的 output 操作： 123456789101112// 完整代码可见本文最后的附录val BLOCK_INTERVAL = 1 // in secondsval BATCH_INTERVAL = 5 // in secondsval CURRENT_JOBS = 10...// DStream DAG 定义开始val inputStream = ssc.receiverStream(...)inputStream.foreachRDD(_ =&gt; Thread.sleep(Int.MaxValue)) // output 1inputStream.foreachRDD(_ =&gt; Thread.sleep(Int.MaxValue)) // output 2// DStream DAG 定义结束... 在上面的设定下，我们很容易知道，能够同时在处理的 batch 有 10 / 2 = 5 个，其余的 batch 的 Job 只能处于等待处理状态。 下面的就是刚才测试代码的运行结果，验证了我们前面的分析和计算： Spark Streaming 的 JobSet, Job，与 Spark Core 的 Job, Stage, TaskSet, Task最后，我们专门拿出一个小节，辨别一下这 Spark Streaming 的 JobSet, Job，与 Spark Core 的 Job, Stage, TaskSet, Task 这几个概念。 [Spark Streaming] JobSet 的全限定名是：org.apache.spark.streaming.scheduler.JobSet Job 的全限定名是：org.apache.spark.streaming.scheduler.Job [Spark Core] Job 没有一个对应的实体类，主要是通过 jobId:Int 来表示一个具体的 job Stage 的全限定名是：org.apache.spark.scheduler.Stage TaskSet 的全限定名是：org.apache.spark.scheduler.TaskSet Task 的全限定名是：org.apache.spark.scheduler.Task Spark Core 的 Job, Stage, Task 就是我们“日常”谈论 Spark 任务时所说的那些含义，而且在 Spark 的 WebUI 上有非常好的体现，比如下图就是 1 个 Job 包含 3 个 Stage；3 个 Stage 各包含 8, 2, 4 个 Task。而 TaskSet 则是 Spark Core 的内部代码里用的类，是 Task 的集合，和 Stage 是同义的。 而 Spark Streaming 里也有一个 Job，但此 Job 非彼 Job。Spark Streaming 里的 Job 更像是个 Java 里的 Runnable，可以 run() 一个自定义的 func 函数。而这个 func, 可以： 直接调用 RDD 的 action，从而产生 1 个或多个 Spark Core 的 Job 先打印一行表头；然后调用 firstTen = RDD.collect()，再打印 firstTen 的内容；最后再打印一行表尾 —— 这正是 DStream.print() 的 Job 实现 也可以是任何用户定义的 code，甚至整个 Spark Streaming 执行过程都不产生任何 Spark Core 的 Job —— 如上一小节所展示的测试代码，其 Job 的 func 实现就是：Thread.sleep(Int.MaxValue)，仅仅是为了让这个 Job 一直跑在 jobExecutor 线程池里，从而测试 jobExecutor 的并行度 :) 最后，Spark Streaming 的 JobSet 就是多个 Job 的集合了。 如果对上面 5 个概念做一个层次划分的话（上一层与下一层多是一对多的关系，但不完全准确），就应该是下表的样子： Spark Core Spark Streaming lv 5 RDD DAGs DStreamGraph lv 4 RDD DAG JobSet lv 3 Job Job lv 2 Stage ← lv 1 Task ← 附录12345678910111213141516171819202122232425262728293031323334353637383940414243444546import java.util.concurrent.&#123;Executors, TimeUnit&#125;import org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.receiver.Receiverimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.SparkConfobject ConcurrentJobsDemo &#123; def main(args: Array[String]) &#123; // 完整代码可见本文最后的附录 val BLOCK_INTERVAL = 1 // in seconds val BATCH_INTERVAL = 5 // in seconds val CURRENT_JOBS = 10 val conf = new SparkConf() conf.setAppName(this.getClass.getSimpleName) conf.setMaster(\"local[2]\") conf.set(\"spark.streaming.blockInterval\", s\"$&#123;BLOCK_INTERVAL&#125;s\") conf.set(\"spark.streaming.concurrentJobs\", s\"$&#123;CURRENT_JOBS&#125;\") val ssc = new StreamingContext(conf, Seconds(BATCH_INTERVAL)) // DStream DAG 定义开始 val inputStream = ssc.receiverStream(new MyReceiver) inputStream.foreachRDD(_ =&gt; Thread.sleep(Int.MaxValue)) // output 1 inputStream.foreachRDD(_ =&gt; Thread.sleep(Int.MaxValue)) // output 2 // DStream DAG 定义结束 ssc.start() ssc.awaitTermination() &#125; class MyReceiver extends Receiver[String](StorageLevel.MEMORY_ONLY) &#123; override def onStart() &#123; // invoke store(\"str\") every 100ms Executors.newScheduledThreadPool(1).scheduleAtFixedRate(new Runnable &#123; override def run(): Unit = store(\"str\") &#125;, 0, 100, TimeUnit.MILLISECONDS) &#125; override def onStop() &#123;&#125; &#125;&#125; （本文完，参与本文的讨论请 猛戳这里，返回目录请 猛戳这里）","categories":[],"tags":[]},{"title":"","slug":"bigdata/spark/Spark Streaming 源码解析系列/1.2 DStream 生成 RDD 实例详解","date":"2018-02-09T03:49:13.888Z","updated":"2018-02-09T03:48:41.410Z","comments":true,"path":"2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/1.2 DStream 生成 RDD 实例详解/","link":"","permalink":"http://yoursite.com/2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/1.2 DStream 生成 RDD 实例详解/","excerpt":"","text":"DStream 生成 RDD 实例详解[酷玩 Spark] Spark Streaming 源码解析系列 ，返回目录请 猛戳这里 「腾讯·广点通」技术团队荣誉出品 12345本系列内容适用范围：* 2017.07.11 update, Spark 2.2 全系列 √ (已发布：2.2.0)* 2017.10.02 update, Spark 2.1 全系列 √ (已发布：2.1.0, 2.1.1, 2.1.2)* 2016.11.14 update, Spark 2.0 全系列 √ (已发布：2.0.0, 2.0.1, 2.0.2) 阅读本文前，请一定先阅读 Spark Streaming 实现思路与模块概述 一文，其中概述了 Spark Streaming 的 4 大模块的基本作用，有了全局概念后再看本文对 模块 1 DAG 静态定义 细节的解释。 引言我们在前面的文章讲过，Spark Streaming 的 模块 1 DAG 静态定义 要解决的问题就是如何把计算逻辑描述为一个 RDD DAG 的“模板”，在后面 Job 动态生成的时候，针对每个 batch，都将根据这个“模板”生成一个 RDD DAG 的实例。 在 Spark Streaming 里，这个 RDD “模板”对应的具体的类是 DStream，RDD DAG “模板”对应的具体类是 DStreamGraph。 DStream 的全限定名是：org.apache.spark.streaming.dstream.DStream DStreamGraph 的全限定名是：org.apache.spark.streaming.DStreamGraph 本文我们就来详解 DStream 最主要的功能：为每个 batch 生成 RDD 实例。 Quick Example我们在前文 DStream, DStreamGraph 详解 中引用了 Spark Streaming 官方的 quick example 的这段对 DStream DAG 的定义，注意看代码中的注释讲解内容： 1234567// ssc.socketTextStream() 将创建一个 SocketInputDStream；这个 InputDStream 的 SocketReceiver 将监听本机 9999 端口val lines = ssc.socketTextStream(\"localhost\", 9999)val words = lines.flatMap(_.split(\" \")) // DStream transformationval pairs = words.map(word =&gt; (word, 1)) // DStream transformationval wordCounts = pairs.reduceByKey(_ + _) // DStream transformationwordCounts.print() // DStream output 这里我们找到 ssc.socketTextStream(&quot;localhost&quot;, 9999) 的源码实现： 123def socketStream[T: ClassTag](hostname: String, port: Int, converter: (InputStream) =&gt; Iterator[T], storageLevel: StorageLevel): ReceiverInputDStream[T] = &#123; new SocketInputDStream[T](this, hostname, port, converter, storageLevel)&#125; 也就是 ssc.socketTextStream() 将 new 出来一个 DStream 具体子类 SocketInputDStream 的实例。 然后我们继续找到下一行 lines.flatMap(_.split(&quot; &quot;)) 的源码实现： 123def flatMap[U: ClassTag](flatMapFunc: T =&gt; Traversable[U]): DStream[U] = ssc.withScope &#123; new FlatMappedDStream(this, context.sparkContext.clean(flatMapFunc))&#125; 也就是 lines.flatMap(_.split(&quot; &quot;)) 将 new 出来一个 DStream 具体子类 FlatMappedDStream 的实例。 后面几行也是如此，所以我们如果用 DStream DAG 图来表示之前那段 quick example 的话，就是这个样子： 也即，我们给出的那段代码，用具体的实现来替换的话，结果如下： 123456val lines = new SocketInputDStream(\"localhost\", 9999) // 类型是 SocketInputDStreamval words = new FlatMappedDStream(lines, _.split(\" \")) // 类型是 FlatMappedDStreamval pairs = new MappedDStream(words, word =&gt; (word, 1)) // 类型是 MappedDStreamval wordCounts = new ShuffledDStream(pairs, _ + _) // 类型是 ShuffledDStreamnew ForeachDStream(wordCounts, cnt =&gt; cnt.print()) // 类型是 ForeachDStream DStream 通过 generatedRDD 管理已生成的 RDDDStream 内部用一个类型是 HashMap 的变量 generatedRDD 来记录已经生成过的 RDD： 1private[streaming] var generatedRDDs = new HashMap[Time, RDD[T]] () generatedRDD 的 key 是一个 Time；这个 Time 是与用户指定的 batchDuration 对齐了的时间 —— 如每 15s 生成一个 batch 的话，那么这里的 key 的时间就是 08h:00m:00s，08h:00m:15s 这种，所以其实也就代表是第几个 batch。generatedRDD 的 value 就是 RDD 的实例。 需要注意，每一个不同的 DStream 实例，都有一个自己的 generatedRDD。如在下图中，DStream a, b, c, d 各有自己的 generatedRDD 变量；图中也示意了 DStream a 的 generatedRDD 变量。 DStream 对这个 HashMap 的存取主要是通过 getOrCompute(time: Time) 方法，实现也很简单，就是一个 —— 查表，如果有就直接返回，如果没有就生成了放入表、再返回 —— 的逻辑： 12345678910111213141516171819202122232425262728293031private[streaming] final def getOrCompute(time: Time): Option[RDD[T]] = &#123; // 从 generatedRDDs 里 get 一下：如果有 rdd 就返回，没有 rdd 就进行 orElse 下面的 rdd 生成步骤 generatedRDDs.get(time).orElse &#123; // 验证 time 需要是 valid if (isTimeValid(time)) &#123; // 然后调用 compute(time) 方法获得 rdd 实例，并存入 rddOption 变量 val rddOption = createRDDWithLocalProperties(time) &#123; PairRDDFunctions.disableOutputSpecValidation.withValue(true) &#123; compute(time) &#125; &#125; rddOption.foreach &#123; case newRDD =&gt; if (storageLevel != StorageLevel.NONE) &#123; newRDD.persist(storageLevel) logDebug(s\"Persisting RDD $&#123;newRDD.id&#125; for time $time to $storageLevel\") &#125; if (checkpointDuration != null &amp;&amp; (time - zeroTime).isMultipleOf(checkpointDuration)) &#123; newRDD.checkpoint() logInfo(s\"Marking RDD $&#123;newRDD.id&#125; for time $time for checkpointing\") &#125; // 将刚刚实例化出来的 rddOption 放入 generatedRDDs 对应的 time 位置 generatedRDDs.put(time, newRDD) &#125; // 返回刚刚实例化出来的 rddOption rddOption &#125; else &#123; None &#125; &#125; &#125; 最主要还是调用了一个 abstract 的 compute(time) 方法。这个方法用于生成 RDD 实例，生成后被放进 generatedRDD 里供后续的查询和使用。这个 compute(time) 方法在 DStream 类里是 abstract 的，但在每个具体的子类里都提供了实现。 (a) InputDStream 的 compute(time) 实现InputDStream 是个有很多子类的抽象类，我们看一个具体的子类 FileInputDStream。 1234567891011121314151617181920// 来自 FileInputDStreamoverride def compute(validTime: Time): Option[RDD[(K, V)]] = &#123; // 通过一个 findNewFiles() 方法，找到 validTime 以后产生的新 file 的数据 val newFiles = findNewFiles(validTime.milliseconds) logInfo(\"New files at time \" + validTime + \":\\n\" + newFiles.mkString(\"\\n\")) batchTimeToSelectedFiles += ((validTime, newFiles)) recentlySelectedFiles ++= newFiles // 找到了一些新 file；以新 file 的数组为参数，通过 filesToRDD() 生成单个 RDD 实例 rdds val rdds = Some(filesToRDD(newFiles)) val metadata = Map( \"files\" -&gt; newFiles.toList, StreamInputInfo.METADATA_KEY_DESCRIPTION -&gt; newFiles.mkString(\"\\n\")) val inputInfo = StreamInputInfo(id, 0, metadata) ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo) // 返回生成的单个 RDD 实例 rdds rdds &#125; 而 filesToRDD() 实现如下： 1234567891011121314151617181920212223// 来自 FileInputDStreamprivate def filesToRDD(files: Seq[String]): RDD[(K, V)] = &#123; // 对每个 file，都 sc.newAPIHadoopFile(file) 来生成一个 RDD val fileRDDs = files.map &#123; file =&gt; val rdd = serializableConfOpt.map(_.value) match &#123; case Some(config) =&gt; context.sparkContext.newAPIHadoopFile( file, fm.runtimeClass.asInstanceOf[Class[F]], km.runtimeClass.asInstanceOf[Class[K]], vm.runtimeClass.asInstanceOf[Class[V]], config) case None =&gt; context.sparkContext.newAPIHadoopFile[K, V, F](file) &#125; if (rdd.partitions.size == 0) &#123; logError(\"File \" + file + \" has no data in it. Spark Streaming can only ingest \" + \"files that have been \\\"moved\\\" to the directory assigned to the file stream. \" + \"Refer to the streaming programming guide for more details.\") &#125; rdd &#125; // 将每个 file 对应的 RDD 进行 union，返回一个 union 后的 UnionRDD new UnionRDD(context.sparkContext, fileRDDs)&#125; 所以，结合以上 compute(validTime: Time) 和 filesToRDD(files: Seq[String]) 方法，我们得出 FileInputDStream 为每个 batch 生成 RDD 的实例过程如下： (1) 先通过一个 findNewFiles() 方法，找到 validTime 以后产生的多个新 file (2) 对每个新 file，都将其作为参数调用 sc.newAPIHadoopFile(file)，生成一个 RDD 实例 (3) 将 (2) 中的多个新 file 对应的多个 RDD 实例进行 union，返回一个 union 后的 UnionRDD 其它 InputDStream 的为每个 batch 生成 RDD 实例的过程也比较类似了。 (b) 一般 DStream 的 compute(time) 实现前一小节的 InputDStream 没有上游依赖的 DStream，可以直接为每个 batch 产生 RDD 实例。一般 DStream 都是由 transofrmation 生成的，都有上游依赖的 DStream，所以为了为 batch 产生 RDD 实例，就需要在 compute(time) 方法里先获取上游依赖的 DStream 产生的 RDD 实例。 具体的，我们看两个具体 DStream —— MappedDStream, FilteredDStream —— 的实现： MappedDStream 的 compute(time) 实现MappedDStream 很简单，全类实现如下： 1234567891011121314151617181920package org.apache.spark.streaming.dstreamimport org.apache.spark.streaming.&#123;Duration, Time&#125;import org.apache.spark.rdd.RDDimport scala.reflect.ClassTagprivate[streaming]class MappedDStream[T: ClassTag, U: ClassTag] ( parent: DStream[T], mapFunc: T =&gt; U ) extends DStream[U](parent.ssc) &#123; override def dependencies: List[DStream[_]] = List(parent) override def slideDuration: Duration = parent.slideDuration override def compute(validTime: Time): Option[RDD[U]] = &#123; parent.getOrCompute(validTime).map(_.map[U](mapFunc)) &#125;&#125; 可以看到，首先在构造函数里传入了两个重要内容： parent，是本 MappedDStream 上游依赖的 DStream mapFunc，是本次 map() 转换的具体函数 在前文 DStream, DStreamGraph 详解 中的 quick example 里的 val pairs = words.map(word =&gt; (word, 1)) 的 mapFunc 就是 word =&gt; (word, 1) 所以在 compute(time) 的具体实现里，就很简单了： (1) 获取 parent DStream 在本 batch 里对应的 RDD 实例 (2) 在这个 parent RDD 实例上，以 mapFunc 为参数调用 .map(mapFunc) 方法，将得到的新 RDD 实例返回 完全相当于用 RDD API 写了这样的代码：return parentRDD.map(mapFunc) FilteredDStream 的 compute(time) 实现再看看 FilteredDStream 的全部实现： 1234567891011121314151617181920package org.apache.spark.streaming.dstreamimport org.apache.spark.streaming.&#123;Duration, Time&#125;import org.apache.spark.rdd.RDDimport scala.reflect.ClassTagprivate[streaming]class FilteredDStream[T: ClassTag]( parent: DStream[T], filterFunc: T =&gt; Boolean ) extends DStream[T](parent.ssc) &#123; override def dependencies: List[DStream[_]] = List(parent) override def slideDuration: Duration = parent.slideDuration override def compute(validTime: Time): Option[RDD[T]] = &#123; parent.getOrCompute(validTime).map(_.filter(filterFunc)) &#125;&#125; 同 MappedDStream 一样，FilteredDStream 也在构造函数里传入了两个重要内容： parent，是本 FilteredDStream 上游依赖的 DStream filterFunc，是本次 filter() 转换的具体函数 所以在 compute(time) 的具体实现里，就很简单了： (1) 获取 parent DStream 在本 batch 里对应的 RDD 实例 (2) 在这个 parent RDD 实例上，以 filterFunc 为参数调用 .filter(filterFunc) 方法，将得到的新 RDD 实例返回 完全相当于用 RDD API 写了这样的代码：return parentRDD.filter(filterFunc) 总结一般 DStream 的 compute(time) 实现总结上面 MappedDStream 和 FilteredDStream 的实现，可以看到： DStream 的 .map() 操作生成了 MappedDStream，而 MappedDStream 在每个 batch 里生成 RDD 实例时，将对 parentRDD 调用 RDD 的 .map() 操作 —— DStream.map() 操作完美复制为每个 batch 的 RDD.map() 操作 DStream 的 .filter() 操作生成了 FilteredDStream，而 FilteredDStream 在每个 batch 里生成 RDD 实例时，将对 parentRDD 调用 RDD 的 .filter() 操作 —— DStream.filter() 操作完美复制为每个 batch 的 RDD.filter() 操作 在最开始， DStream 的 transformation 的 API 设计与 RDD 的 transformation 设计保持了一致，就使得，每一个 dStreamA.transformation() 得到的新 dStreamB 能将 dStreamA.transformation() 操作完美复制为每个 batch 的 rddA.transformation() 操作。 这也就是 DStream 能够作为 RDD 模板，在每个 batch 里实例化 RDD 的根本原因。 (c) ForEachDStream 的 compute(time) 实现上面分析了 DStream 的 transformation 如何在 compute(time) 里复制为 RDD 的 transformation，下面我们分析 DStream 的 output 如何在 compute(time) 里复制为 RDD 的 action。 我们前面讲过，对一个 DStream 进行 output 操作，将生成一个新的 ForEachDStream，这个 ForEachDStream 用一个 foreachFunc 成员来记录 output 的具体内容。 ForEachDStream 全部实现如下： 12345678910111213141516171819202122232425262728293031package org.apache.spark.streaming.dstreamimport org.apache.spark.rdd.RDDimport org.apache.spark.streaming.&#123;Duration, Time&#125;import org.apache.spark.streaming.scheduler.Jobimport scala.reflect.ClassTagprivate[streaming]class ForEachDStream[T: ClassTag] ( parent: DStream[T], foreachFunc: (RDD[T], Time) =&gt; Unit ) extends DStream[Unit](parent.ssc) &#123; override def dependencies: List[DStream[_]] = List(parent) override def slideDuration: Duration = parent.slideDuration override def compute(validTime: Time): Option[RDD[Unit]] = None override def generateJob(time: Time): Option[Job] = &#123; parent.getOrCompute(time) match &#123; case Some(rdd) =&gt; val jobFunc = () =&gt; createRDDWithLocalProperties(time) &#123; ssc.sparkContext.setCallSite(creationSite) foreachFunc(rdd, time) &#125; Some(new Job(time, jobFunc)) case None =&gt; None &#125; &#125;&#125; 同前面一样，ForEachDStream 也在构造函数里传入了两个重要内容： parent，是本 ForEachDStream 上游依赖的 DStream foreachFunc，是本次 output 的具体函数 所以在 compute(time) 的具体实现里，就很简单了： (1) 获取 parent DStream 在本 batch 里对应的 RDD 实例 (2) 以这个 parent RDD 和本次 batch 的 time 为参数，调用 foreachFunc(parentRDD, time) 方法 例如，我们看看 DStream.print() 里 foreachFunc(rdd, time) 的具体实现： 123456789def foreachFunc: (RDD[T], Time) =&gt; Unit = &#123; val firstNum = rdd.take(num + 1) println(\"-------------------------------------------\") println(\"Time: \" + time) println(\"-------------------------------------------\") firstNum.take(num).foreach(println) if (firstNum.length &gt; num) println(\"...\") println()&#125; 就可以知道，如果对着 rdd 调用上面这个 foreachFunc 的话，就会在每个 batch 里，都会在 rdd 上执行 .take() 获取一些元素到 driver 端，然后再 .foreach(println)；也就形成了在 driver 端打印这个 DStream 的一些内容的效果了！ DStreamGraph 生成 RDD DAG 实例在前文 Spark Streaming 实现思路与模块概述 中，我们曾经讲过，在每个 batch 时，都由 JobGenerator 来要求 RDD DAG “模板” 来创建 RDD DAG 实例，即下图中的第 (2) 步。 具体的，是 JobGenerator 来调用 DStreamGraph 的 generateJobs(time) 方法。 那么翻出来 generateJobs() 的实现： 123456789// 来自 DStreamGraphdef generateJobs(time: Time): Seq[Job] = &#123; logDebug(\"Generating jobs for time \" + time) val jobs = this.synchronized &#123; outputStreams.flatMap(outputStream =&gt; outputStream.generateJob(time)) &#125; logDebug(\"Generated \" + jobs.length + \" jobs for time \" + time) jobs&#125; 也就是说，是 DStreamGraph 继续调用了每个 outputStream 的 generateJob(time) 方法 —— 而我们知道，只有 ForEachDStream 是 outputStream，所以将调用 ForEachDStream 的 generateJob(time) 方法。 举个例子，如上图，由于我们在代码里的两次 print() 操作产生了两个 ForEachDStream 节点 x 和 y，那么 DStreamGraph.generateJobs(time) 就将先后调用 x.generateJob(time) 和 y.generateJob(time) 方法，并将各获得一个 Job。 但是…… x.generateJob(time) 和 y.generateJob(time) 的返回值 Job 到底是啥？那我们先插播一下 Job。 Spark Streaming 的 JobSpark Streaming 里重新定义了一个 Job 类，功能与 Java 的 Runnable 差不多：一个 Job 能够自定义一个 func() 函数，而 Job 的 .run() 方法实现就是执行这个 func()。 1234567891011// 节选自 org.apache.spark.streaming.scheduler.Jobprivate[streaming]class Job(val time: Time, func: () =&gt; _) &#123; ... def run() &#123; _result = Try(func()) &#125; ...&#125; 所以其实 Job 的本质是将实际的 func() 定义和 func() 被调用分离了 —— 就像 Runnable 是将 run() 的具体定义和 run() 的被调用分离了一样。 下面我们继续来看 x.generateJob(time) 和 y.generateJob(time) 实现。 x.generateJob(time) 过程x 是一个 ForEachDStream，其 generateJob(time) 的实现如下： 123456789101112131415// 来自 ForEachDStreamoverride def generateJob(time: Time): Option[Job] = &#123; // 【首先调用 parentDStream 的 getOrCompute() 来获取 parentRDD】 parent.getOrCompute(time) match &#123; case Some(rdd) =&gt; // 【然后定义 jobFunc 为在 parentRDD 上执行 foreachFun() 】 val jobFunc = () =&gt; createRDDWithLocalProperties(time) &#123; ssc.sparkContext.setCallSite(creationSite) foreachFunc(rdd, time) &#125; // 【最后将 jobFunc 包装为 Job 返回】 Some(new Job(time, jobFunc)) case None =&gt; None &#125;&#125; 就是这里牵扯到了 x 的 parentDStream.getOrCompute(time)，即 d.getOrCompute(time)；而 d.getOrCompute(time) 会牵扯 c.getOrCompute(time)，乃至 a.getOrCompute(time), b.getOrCompute(time) 用一个时序图来表达这里的调用关系会清晰很多： 所以最后的时候，由于对 x.generateJob(time) 形成的递归调用， 将形成一个 Job，其内容 func 如下图： y.generateJob(time) 过程同样的，y 节点生成 Job 的过程，与 x 节点的过程非常类似，只是在 b.getOrCompute(time) 时，会命中 get(time) 而不需要触发 compute(time) 了，这是因为该 RDD 实例已经在 x 节点的生成过程中被实例化过一次，所以在这里只需要取出来用就可以了。 同样，最后的时候，由于对 y.generateJob(time) 形成的递归调用， 将形成一个 Job，其内容 func 如下图： 返回 Seq[Job]所以当 DStreamGraph.generateJobs(time) 结束时，会返回多个 Job，是因为作为 output stream 的每个 ForEachDStream 都通过 generateJob(time) 方法贡献了一个 Job。 比如在上图里，DStreamGraph.generateJobs(time) 会返回一个 Job 的序列，其大小为 2，其内容分别为： 至此，在给定的 batch 里，DStreamGraph.generateJobs(time) 的工作已经全部完成，Seq[Job] 作为结果返回给 JobGenerator 后，JobGenerator 也会尽快提交到 JobSheduler 那里尽快调用 Job.run() 使得这 2 个 RDD DAG 尽快运行起来。 而且，每个新 batch 生成时，都会调用 DStreamGraph.generateJobs(time)，也进而触发我们之前讨论这个 Job 生成过程，周而复始。 到此，整个 DStream 作为 RDD 的 “模板” 为每个 batch 实例化 RDD，DStreamGraph 作为 RDD DAG 的 “模板” 为每个 batch 实例化 RDD DAG，就分析完成了。 （本文完，参与本文的讨论请 猛戳这里，返回目录请 猛戳这里）","categories":[],"tags":[]},{"title":"","slug":"bigdata/spark/Spark Streaming 源码解析系列/1.1 DStream, DStreamGraph 详解","date":"2018-02-09T03:49:13.878Z","updated":"2018-02-09T03:48:41.409Z","comments":true,"path":"2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/1.1 DStream, DStreamGraph 详解/","link":"","permalink":"http://yoursite.com/2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/1.1 DStream, DStreamGraph 详解/","excerpt":"","text":"DStream, DStreamGraph 详解[酷玩 Spark] Spark Streaming 源码解析系列 ，返回目录请 猛戳这里 「腾讯·广点通」技术团队荣誉出品 12345本系列内容适用范围：* 2017.07.11 update, Spark 2.2 全系列 √ (已发布：2.2.0)* 2017.10.02 update, Spark 2.1 全系列 √ (已发布：2.1.0, 2.1.1, 2.1.2)* 2016.11.14 update, Spark 2.0 全系列 √ (已发布：2.0.0, 2.0.1, 2.0.2) 阅读本文前，请一定先阅读 Spark Streaming 实现思路与模块概述 一文，其中概述了 Spark Streaming 的 4 大模块的基本作用，有了全局概念后再看本文对 模块 1 DAG 静态定义 细节的解释。 引言我们在前面的文章讲过，Spark Streaming 的 模块 1 DAG 静态定义 要解决的问题就是如何把计算逻辑描述为一个 RDD DAG 的“模板”，在后面 Job 动态生成的时候，针对每个 batch，都将根据这个“模板”生成一个 RDD DAG 的实例。 在 Spark Streaming 里，这个 RDD “模板”对应的具体的类是 DStream，RDD DAG “模板”对应的具体类是 DStreamGraph。 DStream 的全限定名是：org.apache.spark.streaming.dstream.DStream DStreamGraph 的全限定名是：org.apache.spark.streaming.DStreamGraph 本文涉及的类在 Spark Streaming 中的位置如上图所示；下面详解 DStream, DStreamGraph。 DStream, transformation, output operation 解析回想一下，RDD 的定义是一个只读、分区的数据集（an RDD is a read-only, partitioned collection of records），而 DStream 又是 RDD 的模板，所以我们把 Dstream 也视同数据集。 我们先看看定义在这个 DStream 数据集上的转换（transformation）和 输出（output）。 现在假设我们有一个 DStream 数据集 a： 1val a = new DStream() 那么通过 filter() 操作就可以从 a 生成一个新的 DStream 数据集 b： 1val b = a.filter(func) 这里能够由已有的 DStream 产生新 DStream 的操作统称 transformation。一些典型的 tansformation 包括 map(), filter(), reduce(), join() 等 。 Transformation Meaningmap(func) Return a new DStream by passing each element of the source DStream through a function func.flatMap(func) Similar to map, but each input item can be mapped to 0 or more output items.filter(func) Return a new DStream by selecting only the records of the source DStream on which func returns true.repartition(numPartitions) Changes the level of parallelism in this DStream by creating more or fewer partitions. 另一些不产生新 DStream 数据集，而是只在已有 DStream 数据集上进行的操作和输出，统称为 output。比如 a.print() 就不会产生新的数据集，而是只是将 a 的内容打印出来，所以 print() 就是一种 output 操作。一些典型的 output 包括 print(), saveAsTextFiles(), saveAsHadoopFiles(), foreachRDD() 等。 print() Prints the first ten elements of every batch of data in a DStream on the driver node running the streaming application. This is useful for development and debugging.Python API This is called pprint() in the Python API.saveAsTextFiles(prefix, [suffix]) Save this DStream’s contents as text files. The file name at each batch interval is generated based on prefix and suffix: “prefix-TIME_IN_MS[.suffix]”.saveAsObjectFiles(prefix, [suffix]) Save this DStream’s contents as SequenceFiles of serialized Java objects. The file name at each batch interval is generated based on prefix and suffix: “prefix-TIME_IN_MS[.suffix]”.Python API This is not available in the Python API.saveAsHadoopFiles(prefix, [suffix]) Save this DStream’s contents as Hadoop files. The file name at each batch interval is generated based on prefix and suffix: “prefix-TIME_IN_MS[.suffix]”.Python API This is not available in the Python API.foreachRDD(func) The most generic output operator that applies a function, func, to each RDD generated from the stream. This function should push the data in each RDD to an external system, such as saving the RDD to files, or writing it over the network to a database. Note that the function func is executed in the driver process running the streaming application, and will usually have RDD actions in it that will force the computation of the streaming RDDs. 一段 quick example 的 transformation, output 解析我们看一下 Spark Streaming 官方的 quick example 的这段对 DStream DAG 的定义，注意看代码中的注释讲解内容： 1234567// ssc.socketTextStream() 将创建一个 SocketInputDStream；这个 InputDStream 的 SocketReceiver 将监听本机 9999 端口val lines = ssc.socketTextStream(\"localhost\", 9999)val words = lines.flatMap(_.split(\" \")) // DStream transformationval pairs = words.map(word =&gt; (word, 1)) // DStream transformationval wordCounts = pairs.reduceByKey(_ + _) // DStream transformationwordCounts.print() // DStream output 这里我们找到 ssc.socketTextStream(&quot;localhost&quot;, 9999) 的源码实现： 12345678def socketStream[T: ClassTag]( hostname: String, port: Int, converter: (InputStream) =&gt; Iterator[T], storageLevel: StorageLevel) : ReceiverInputDStream[T] = &#123; new SocketInputDStream[T](this, hostname, port, converter, storageLevel) &#125; 也就是 ssc.socketTextStream() 将 new 出来一个 DStream 具体子类 SocketInputDStream 的实例。 然后我们继续找到下一行 lines.flatMap(_.split(&quot; &quot;)) 的源码实现： 123def flatMap[U: ClassTag](flatMapFunc: T =&gt; Traversable[U]): DStream[U] = ssc.withScope &#123; new FlatMappedDStream(this, context.sparkContext.clean(flatMapFunc)) &#125; 也就是 lines.flatMap(_.split(&quot; &quot;)) 将 new 出来一个 DStream 具体子类 FlatMappedDStream 的实例。 后面几行也是如此，所以我们如果用 DStream DAG 图来表示之前那段 quick example 的话，就是这个样子： 也即，我们给出的那段代码，用具体的实现来替换的话，结果如下： 123456val lines = new SocketInputDStream(\"localhost\", 9999) // 类型是 SocketInputDStreamval words = new FlatMappedDStream(lines, _.split(\" \")) // 类型是 FlatMappedDStreamval pairs = new MappedDStream(words, word =&gt; (word, 1)) // 类型是 MappedDStreamval wordCounts = new ShuffledDStream(pairs, _ + _) // 类型是 ShuffledDStreamnew ForeachDStream(wordCounts, cnt =&gt; cnt.print()) // 类型是 ForeachDStream 总结一下： transformation：可以看到基本上 1 种 transformation 将对应产生一个新的 DStream 子类实例，如： .flatMap() 将产生 FaltMappedDStream 实例 .map() 将产生 MappedDStream 实例 output：将只产生一种 ForEachDStream 子类实例，用一个函数 func 来记录需要做的操作 如对于 print() 就是：func = cnt =&gt; cnt.print() 我们将有另一篇文章具体对 DStream 所有 transformation 的列举和分析，本文不展开。 DStream 类继承体系上面我们看到的 SocketInputDStream, FlatMappedDStream, ForeachDStream 等都是 DStream 的具体子类。 DStream 的所有子类如下： 一会我们要对其这些 DStream 子类进行一个分类。 Dependency, DStreamGraph 解析先再次回过头来看一下 transformation 操作。当我们写代码 c = a.join(b), d = c.filter() 时， 它们的 DAG 逻辑关系是 a/b → c，c → d，但在 Spark Streaming 在进行物理记录时却是反向的 a/b ← c, c ← d，如下图： 那物理上为什么不顺着 DAG 来正向记录，却用反向记录？ 这里背后的原因是，在 Spark Core 的 RDD API 里，RDD 的计算是被触发了以后才进行 lazy 求值的，即当真正求 d 的值的时候，先计算上游 dependency c；而计算 c 则先进一步计算 c 的上游 dependency a 和 b。Spark Streaming 里则与 RDD DAG 的反向表示保持了一致，对 DStream 也采用的反向表示。 所以，这里 d 对 c 的引用，表达的是一个上游依赖（dependency）的关系；也就是说，不求值则已，一旦 d.print() 这个 output 操作触发了对 d 的求值，那么就需要从 d 开始往上游进行追溯计算。 具体的过程是，d.print() 将 new 一个 d 的一个下游 ForEachDStream x —— x 中记明了需要做的操作 func = print() —— 然后在每个 batch 动态生成 RDD 实例时，以 x 为根节点、进行一次 BFS（宽度优先遍历），就可以快速得到需要进行实际计算的最小集合。如下图所示，这个最小集合就是 {a, b, c, d}。 再看一个例子。如下图所示，如果对 d, f 分别调用 print() 的 output 操作，那么将在 d, f 的下游分别产生新的 DStream x, y，分别记录了具体操作 func = print()。在每个 batch 动态生成 RDD 实例时，就会分别对 x 和 y 进行 BFS 遍历，分别得到上游集合 {a,b,c,d} 和 {b,e,f}。作为对比，这里我们不对 h 进行 print() 的 output 操作，所以 g, h 将得不到遍历。 通过以上分析，我们总结一下： (1) DStream 逻辑上通过 transformation 来形成 DAG，但在物理上却是通过与 transformation 反向的依赖（dependency）来构成表示的 (2) 当某个节点调用了 output 操作时，就产生一个新的 ForEachDStream ，这个新的 ForEachDStream 记录了具体的 output 操作是什么 (3) 在每个 batch 动态生成 RDD 实例时，就对 (2) 中新生成的 DStream 进行 BFS 遍历 我们将在 (2) 中，由 output 操作新生成的 DStream 称为 output stream。 最后，我们给出： (4) Spark Streaming 记录整个 DStream DAG 的方式，就是通过一个 DStreamGraph 实例记录了到所有的 output stream 节点的引用 通过对所有 output stream 节点进行遍历，就可以得到所有上游依赖的 DStream 不能被遍历到的 DStream 节点 —— 如 g 和 h —— 则虽然出现在了逻辑的 DAG 中，但是并不属于物理的 DStreamGraph，也将在 Spark Streaming 的实际运行过程中不产生任何作用 (5) DStreamGraph 实例同时也记录了到所有 input stream 节点的引用 DStreamGraph 时常需要遍历没有上游依赖的 DStream 节点 —— 称为 input stream —— 记录一下就可以避免每次为查找 input stream 而对 output steam 进行 BFS 的消耗 我们本节所描述的内容，用下图就能够总结了： （本文完，参与本文的讨论请 猛戳这里，返回目录请 猛戳这里）","categories":[],"tags":[]},{"title":"","slug":"bigdata/spark/Spark Streaming 源码解析系列/0.1 Spark Streaming 实现思路与模块概述","date":"2018-02-09T03:49:13.868Z","updated":"2018-02-09T03:48:41.379Z","comments":true,"path":"2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/0.1 Spark Streaming 实现思路与模块概述/","link":"","permalink":"http://yoursite.com/2018/02/09/bigdata/spark/Spark Streaming 源码解析系列/0.1 Spark Streaming 实现思路与模块概述/","excerpt":"","text":"Spark Streaming 实现思路与模块概述[酷玩 Spark] Spark Streaming 源码解析系列 ，返回目录请 猛戳这里 「腾讯·广点通」技术团队荣誉出品 12345本系列内容适用范围：* 2017.07.11 update, Spark 2.2 全系列 √ (已发布：2.2.0)* 2017.10.02 update, Spark 2.1 全系列 √ (已发布：2.1.0, 2.1.1, 2.1.2)* 2016.11.14 update, Spark 2.0 全系列 √ (已发布：2.0.0, 2.0.1, 2.0.2) 一、基于 Spark 做 Spark Streaming 的思路Spark Streaming 与 Spark Core 的关系可以用下面的经典部件图来表述： 在本节，我们先探讨一下基于 Spark Core 的 RDD API，如何对 streaming data 进行处理。理解下面描述的这个思路非常重要，因为基于这个思路详细展开后，就能够充分理解整个 Spark Streaming 的模块划分和代码逻辑。 第一步，假设我们有一小块数据，那么通过 RDD API，我们能够构造出一个进行数据处理的 RDD DAG（如下图所示）。 第二步，我们对连续的 streaming data 进行切片处理 —— 比如将最近 200ms 时间的 event 积攒一下 —— 每个切片就是一个 batch，然后使用第一步中的 RDD DAG 对这个 batch 的数据进行处理。 注意: 这里我们使用的是 batch 的概念 —— 其实 200ms 在其它同类系统中通常叫做 mini-batch，不过既然 Spark Streaming 官方的叫法就是 batch，我们这里就用 batch 表达 mini-batch 的意思了 :) 所以，针对连续不断的 streaming data 进行多次切片，就会形成多个 batch，也就对应出来多个 RDD DAG（每个 RDD DAG 针对一个 batch 的数据）。如此一来，这多个 RDD DAG 之间相互同构，却又是不同的实例。我们用下图来表示这个关系： 所以，我们将需要： (1) 一个静态的 RDD DAG 的模板，来表示处理逻辑； (2) 一个动态的工作控制器，将连续的 streaming data 切分数据片段，并按照模板复制出新的 RDD DAG 的实例，对数据片段进行处理； 第三步，我们回过头来看 streaming data 本身的产生。Hadoop MapReduce, Spark RDD API 进行批处理时，一般默认数据已经在 HDFS, HBase 或其它存储上。而 streaming data —— 比如 twitter 流 —— 又有可能是在系统外实时产生的，就需要能够将这些数据导入到 Spark Streaming 系统里，就像 Apache Storm 的 Spout，Apache S4 的 Adapter 能够把数据导入系统里的作用是一致的。所以，我们将需要： (3) 原始数据的产生和导入； 第四步，我们考虑，有了以上 (1)(2)(3) 3 部分，就可以顺利用 RDD API 处理 streaming data 了吗？其实相对于 batch job 通常几个小时能够跑完来讲，streaming job 的运行时间是 +∞（正无穷大）的，所以我们还将需要： (4) 对长时运行任务的保障，包括输入数据的失效后的重构，处理任务的失败后的重调。 至此，streaming data 的特点决定了，如果我们想基于 Spark Core 进行 streaming data 的处理，还需要在 Spark Core 的框架上解决刚才列出的 (1)(2)(3)(4) 这四点问题： 二、Spark Streaming 的整体模块划分根据 Spark Streaming 解决这 4 个问题的不同 focus，可以将 Spark Streaming 划分为四个大的模块： 模块 1：DAG 静态定义 模块 2：Job 动态生成 模块 3：数据产生与导入 模块 4：长时容错 其中每个模块涉及到的主要的类，示意如下： 这里先不用纠结每个类的具体用途，我们将在本文中简述，并在本系列的后续文章里对每个模块逐一详述。 2.1 模块 1：DAG 静态定义通过前面的描述我们知道，应该首先对计算逻辑描述为一个 RDD DAG 的“模板”，在后面 Job 动态生成的时候，针对每个 batch，Spark Streaming 都将根据这个“模板”生成一个 RDD DAG 的实例。 DStream 和 DStreamGraph其实在 Spark Streaming 里，这个 RDD “模板”对应的具体的类是 DStream，RDD DAG “模板”对应的具体类是 DStreamGraph。而 RDD 本身也有很多子类，几乎每个子类都有一个对应的 DStream，如 UnionRDD 的对应是 UnionDStream。RDD 通过 transformation 连接成 RDD DAG（但 RDD DAG 在 Spark Core 里没有对应的具体类），DStream 也通过 transformation 连接成 DStreamGraph。 DStream 的全限定名是：org.apache.spark.streaming.dstream.DStream DStreamGraph 的全限定名是：org.apache.spark.streaming.DStreamGraph DStream 和 RDD 的关系既然 DStream 是 RDD 的模板，而且 DStream 和 RDD 具有相同的 transformation 操作，比如 map(), filter(), reduce() ……等等（正是这些相同的 transformation 使得 DStreamGraph 能够忠实记录 RDD DAG 的计算逻辑），那 RDD 和 DStream 有什么不一样吗？ 还真不一样。 比如，DStream 维护了对每个产出的 RDD 实例的引用。比如下图里，DStream A 在 3 个 batch 里分别实例化了 3 个 RDD，分别是 a[1], a[2], a[3]，那么 DStream A 就保留了一个 batch → 所产出的 RDD 的哈希表，即包含 batch 1 → a[1], batch 2 → a[2], batch 3 → a[3] 这 3 项。 另外，能够进行流量控制的 DStream 子类，如 ReceiverInputDStream，还会保存关于历次 batch 的源头数据条数、历次 batch 计算花费的时间等数值，用来实时计算准确的流量控制信息，这些都是记在 DStream 里的，而 RDD a[1] 等则不会保存这些信息。 我们在考虑的时候，可以认为，RDD 加上 batch 维度就是 DStream，DStream 去掉 batch 维度就是 RDD —— 就像 RDD = DStream at batch T。 不过这里需要特别说明的是，在 DStreamGraph 的图里，DStream（即数据）是顶点，DStream 之间的 transformation（即计算）是边，这与 Apache Storm 等是相反的。 在 Apache Storm 的 topology 里，计算是顶点，stream（连续的 tuple，即数据）是边。这一点也是比较熟悉 Storm 的同学刚开始一下子不太理解 DStream 的原因–我们再重复一遍，DStream 即是数据本身，在有向图里是顶点、而不是边。 2.2 模块 2：Job 动态生成现在有了 DStreamGraph 和 DStream，也就是静态定义了的计算逻辑，下面我们来看 Spark Streaming 是如何将其动态调度的。 在 Spark Streaming 程序的入口，我们都会定义一个 batchDuration，就是需要每隔多长时间就比照静态的 DStreamGraph 来动态生成一个 RDD DAG 实例。在 Spark Streaming 里，总体负责动态作业调度的具体类是 JobScheduler，在 Spark Streaming 程序开始运行的时候，会生成一个 JobScheduler 的实例，并被 start() 运行起来。 JobScheduler 有两个非常重要的成员：JobGenerator 和 ReceiverTracker。JobScheduler 将每个 batch 的 RDD DAG 具体生成工作委托给 JobGenerator，而将源头输入数据的记录工作委托给 ReceiverTracker。 JobScheduler 的全限定名是：org.apache.spark.streaming.scheduler.JobScheduler JobGenerator 的全限定名是：org.apache.spark.streaming.scheduler.JobGenerator ReceiverTracker 的全限定名是：org.apache.spark.streaming.scheduler.ReceiverTracker JobGenerator 维护了一个定时器，周期就是我们刚刚提到的 batchDuration，定时为每个 batch 生成 RDD DAG 的实例。具体的，每次 RDD DAG 实际生成包含 5 个步骤： (1) 要求 ReceiverTracker 将目前已收到的数据进行一次 allocate，即将上次 batch 切分后的数据切分到到本次新的 batch 里； (2) 要求 DStreamGraph 复制出一套新的 RDD DAG 的实例，具体过程是：DStreamGraph 将要求图里的尾 DStream 节点生成具体的 RDD 实例，并递归的调用尾 DStream 的上游 DStream 节点……以此遍历整个 DStreamGraph，遍历结束也就正好生成了 RDD DAG 的实例； (3) 获取第 1 步 ReceiverTracker 分配到本 batch 的源头数据的 meta 信息； (4) 将第 2 步生成的本 batch 的 RDD DAG，和第 3 步获取到的 meta 信息，一同提交给 JobScheduler 异步执行； (5) 只要提交结束（不管是否已开始异步执行），就马上对整个系统的当前运行状态做一个 checkpoint。 上述 5 个步骤的调用关系图如下： 2.3 模块 3：数据产生与导入下面我们看 Spark Streaming 解决第三个问题的模块分析，即数据的产生与导入。 DStream 有一个重要而特殊的子类 ReceiverInputDStream：它除了需要像其它 DStream 那样在某个 batch 里实例化 RDD 以外，还需要额外的 Receiver 为这个 RDD 生产数据！ 具体的，Spark Streaming 在程序刚开始运行时： (1) 由 Receiver 的总指挥 ReceiverTracker 分发多个 job（每个 job 有 1 个 task），到多个 executor 上分别启动 ReceiverSupervisor 实例； (2) 每个 ReceiverSupervisor 启动后将马上生成一个用户提供的 Receiver 实现的实例 —— 该 Receiver 实现可以持续产生或者持续接收系统外数据，比如 TwitterReceiver 可以实时爬取 twitter 数据 —— 并在 Receiver 实例生成后调用 Receiver.onStart()； ReceiverSupervisor 的全限定名是：org.apache.spark.streaming.receiver.ReceiverSupervisor Receiver 的全限定名是：org.apache.spark.streaming.receiver.Receiver (1)(2) 的过程由上图所示，这时 Receiver 启动工作已运行完毕。 接下来 ReceiverSupervisor 将在 executor 端作为的主要角色，并且： (3) Receiver 在 onStart() 启动后，就将持续不断地接收外界数据，并持续交给 ReceiverSupervisor 进行数据转储； (4) ReceiverSupervisor 持续不断地接收到 Receiver 转来的数据： 如果数据很细小，就需要 BlockGenerator 攒多条数据成一块(4a)、然后再成块存储(4b 或 4c) 反之就不用攒，直接成块存储(4b 或 4c) 这里 Spark Streaming 目前支持两种成块存储方式，一种是由 BlockManagerBasedBlockHandler 直接存到 executor 的内存或硬盘，另一种由 WriteAheadLogBasedBlockHandler 是同时写 WAL(4c) 和 executor 的内存或硬盘 (5) 每次成块在 executor 存储完毕后，ReceiverSupervisor 就会及时上报块数据的 meta 信息给 driver 端的 ReceiverTracker；这里的 meta 信息包括数据的标识 id，数据的位置，数据的条数，数据的大小等信息； (6) ReceiverTracker 再将收到的块数据 meta 信息直接转给自己的成员 ReceivedBlockTracker，由 ReceivedBlockTracker 专门管理收到的块数据 meta 信息。 BlockGenerator 的全限定名是：org.apache.spark.streaming.receiver.BlockGenerator BlockManagerBasedBlockHandler 的全限定名是：org.apache.spark.streaming.receiver.BlockManagerBasedBlockHandler WriteAheadLogBasedBlockHandler 的全限定名是：org.apache.spark.streaming.receiver.WriteAheadLogBasedBlockHandler ReceivedBlockTracker 的全限定名是：org.apache.spark.streaming.scheduler.ReceivedBlockTracker ReceiverInputDStream 的全限定名是：org.apache.spark.streaming.dstream.ReceiverInputDStream 这里 (3)(4)(5)(6) 的过程是一直持续不断地发生的，我们也将其在上图里标识出来。 后续在 driver 端，就由 ReceiverInputDStream 在每个 batch 去检查 ReceiverTracker 收到的块数据 meta 信息，界定哪些新数据需要在本 batch 内处理，然后生成相应的 RDD 实例去处理这些块数据，这个过程在模块 1：DAG 静态定义 模块2：Job 动态生成 里描述过了。 2.4 模块 4：长时容错以上我们简述完成 Spark Streamimg 基于 Spark Core 所新增功能的 3 个模块，接下来我们看一看第 4 个模块将如何保障 Spark Streaming 的长时运行 —— 也就是，如何与前 3 个模块结合，保障前 3 个模块的长时运行。 通过前 3 个模块的关键类的分析，我们可以知道，保障模块 1 和 2 需要在 driver 端完成，保障模块 3 需要在 executor 端和 driver 端完成。 executor 端长时容错先看 executor 端。 在 executor 端，ReceiverSupervisor 和 Receiver 失效后直接重启就 OK 了，关联是保障收到的块数据的安全。保障了源头块数据，就能够保障 RDD DAG （Spark Core 的 lineage）重做。 Spark Streaming 对源头块数据的保障，分为 4 个层次，全面、相互补充，又可根据不同场景灵活设置： (1) 热备：热备是指在存储块数据时，将其存储到本 executor、并同时 replicate 到另外一个 executor 上去。这样在一个 replica 失效后，可以立刻无感知切换到另一份 replica 进行计算。实现方式是，在实现自己的 Receiver 时，即指定一下 StorageLevel 为 MEMORY_ONLY_2 或 MEMORY_AND_DISK_2 就可以了。 // 1.5.2 update 这已经是默认了。 (2) 冷备：冷备是每次存储块数据前，先把块数据作为 log 写出到 WriteAheadLog 里，再存储到本 executor。executor 失效时，就由另外的 executor 去读 WAL，再重做 log 来恢复块数据。WAL 通常写到可靠存储如 HDFS 上，所以恢复时可能需要一段 recover time。 (3) 重放：如果上游支持重放，比如 Apache Kafka，那么就可以选择不用热备或者冷备来另外存储数据了，而是在失效时换一个 executor 进行数据重放即可。 (4) 忽略：最后，如果应用的实时性需求大于准确性，那么一块数据丢失后我们也可以选择忽略、不恢复失效的源头数据。 我们用一个表格来总结一下： 图示 优点 缺点 (1) 热备 无 recover time 需要占用双倍资源 (2) 冷备 十分可靠 存在 recover time (3) 重放 不占用额外资源 存在 recover time (4) 忽略 无 recover time 准确性有损失 driver 端长时容错前面我们讲过，块数据的 meta 信息上报到 ReceiverTracker，然后交给 ReceivedBlockTracker 做具体的管理。ReceivedBlockTracker 也采用 WAL 冷备方式进行备份，在 driver 失效后，由新的 ReceivedBlockTracker 读取 WAL 并恢复 block 的 meta 信息。 另外，需要定时对 DStreamGraph 和 JobScheduler 做 Checkpoint，来记录整个 DStreamGraph 的变化、和每个 batch 的 job 的完成情况。 注意到这里采用的是完整 checkpoint 的方式，和之前的 WAL 的方式都不一样。Checkpoint 通常也是落地到可靠存储如 HDFS。Checkpoint 发起的间隔默认的是和 batchDuration 一致；即每次 batch 发起、提交了需要运行的 job 后就做 Checkpoint，另外在 job 完成了更新任务状态的时候再次做一下 Checkpoint。 这样一来，在 driver 失效并恢复后，可以读取最近一次的 Checkpoint 来恢复作业的 DStreamGraph 和 job 的运行及完成状态。 总结 模块 长时容错保障方式 模块 1-DAG 静态定义 driver 端 定时对 DStreamGraph 做 Checkpoint，来记录整个 DStreamGraph 的变化 模块 2-job 动态生成 driver 端 定时对 JobScheduler 做 Checkpoint，来记录每个 batch 的 job 的完成情况 模块 3-数据产生与导入 driver 端 源头块数据的 meta 信息上报 ReceiverTracker 时，写入 WAL 模块 3-数据产生与导入 executor 端 对源头块数据的保障：(1) 热备；(2) 冷备；(3) 重放；(4) 忽略 总结一下“模块4：长时容错”的内容为上述表格，可以看到，Spark Streaming 的长时容错特性，能够提供不重、不丢，exactly-once 的处理语义。 三、入口：StreamingContext上面我们花了很多篇幅来介绍 Spark Streaming 的四大模块，我们在最后介绍一下 StreamingContext。 下面我们用这段仅 11 行的完整 quick example，来说明用户 code 是怎么通过 StreamingContext 与前面几个模块进行交互的： 1234567891011121314151617181920212223242526272829import org.apache.spark._import org.apache.spark.streaming._// 首先配置一下本 quick example 将跑在本机，app name 是 NetworkWordCountval conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"NetworkWordCount\")// batchDuration 设置为 1 秒，然后创建一个 streaming 入口val ssc = new StreamingContext(conf, Seconds(1))// ssc.socketTextStream() 将创建一个 SocketInputDStream；这个 InputDStream 的 SocketReceiver 将监听本机 9999 端口val lines = ssc.socketTextStream(\"localhost\", 9999)val words = lines.flatMap(_.split(\" \")) // DStream transformationval pairs = words.map(word =&gt; (word, 1)) // DStream transformationval wordCounts = pairs.reduceByKey(_ + _) // DStream transformationwordCounts.print() // DStream output// 上面 4 行利用 DStream transformation 构造出了 lines -&gt; words -&gt; pairs -&gt; wordCounts -&gt; .print() 这样一个 DStreamGraph// 但注意，到目前是定义好了产生数据的 SocketReceiver，以及一个 DStreamGraph，这些都是静态的// 下面这行 start() 将在幕后启动 JobScheduler, 进而启动 JobGenerator 和 ReceiverTracker// ssc.start()// -&gt; JobScheduler.start()// -&gt; JobGenerator.start(); 开始不断生成一个一个 batch// -&gt; ReceiverTracker.start(); 开始往 executor 上分布 ReceiverSupervisor 了，也会进一步创建和启动 Receiverssc.start()// 然后用户 code 主线程就 block 在下面这行代码了// block 的后果就是，后台的 JobScheduler 线程周而复始的产生一个一个 batch 而不停息// 也就是在这里，我们前面静态定义的 DStreamGraph 的 print()，才一次一次被在 RDD 实例上调用，一次一次打印出当前 batch 的结果ssc.awaitTermination() 所以我们看到，StreamingContext 是 Spark Streaming 提供给用户 code 的、与前述 4 个模块交互的一个简单和统一的入口。 四、总结与回顾在最后我们再把 [Sark Streaming 官方 Programming Guide] (http://spark.apache.org/docs/latest/streaming-programming-guide.html#a-quick-example) 的部分内容放在这里，作为本文的一个回顾和总结。请大家看一看，如果看懂了本文的内容，是不是读下面这些比较 high-level 的介绍会清晰化很多 :-) Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams. Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches. Spark Streaming provides a high-level abstraction called discretized stream or DStream, which represents a continuous stream of data. DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level operations on other DStreams. Internally, a DStream is represented as a sequence of RDDs. … ##知识共享 除非另有注明，本文及本《Spark Streaming 源码解析系列》系列文章使用 CC BY-NC（署名-非商业性使用） 知识共享许可协议。 （本文完，参与本文的讨论请 猛戳这里，返回目录请 猛戳这里）","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2018-01-29T01:52:14.136Z","updated":"2018-01-29T01:52:14.136Z","comments":true,"path":"2018/01/29/hello-world/","link":"","permalink":"http://yoursite.com/2018/01/29/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}